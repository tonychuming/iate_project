{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.backends.cudnn as cudnn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import timm\n",
    "import numpy as np\n",
    "from torchvision import transforms\n",
    "from torch.optim.lr_scheduler import CosineAnnealingLR, ReduceLROnPlateau\n",
    "from PIL import Image\n",
    "import random\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import (\n",
    "    classification_report,\n",
    "    confusion_matrix,\n",
    "    accuracy_score,\n",
    "    precision_score,\n",
    "    recall_score,\n",
    "    f1_score,\n",
    "    roc_auc_score\n",
    ")\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "import logging\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "import gc\n",
    "from collections import Counter\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from torch.amp import autocast, GradScaler\n",
    "\n",
    "try:\n",
    "    import psutil\n",
    "    PSUTIL_AVAILABLE = True\n",
    "except ImportError:\n",
    "    PSUTIL_AVAILABLE = False\n",
    "\n",
    "# Logging Configuration\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(levelname)s - %(message)s',\n",
    "    handlers=[\n",
    "        logging.StreamHandler()\n",
    "    ]\n",
    ")\n",
    "logger = logging.getLogger('DLTraining')\n",
    "\n",
    "# Global Config Variables\n",
    "BASE_PATH = Path(r\"D:\\iate_project\")\n",
    "DATA_DIR = BASE_PATH / \"original_dataset\"\n",
    "OUTPUT_DIR = BASE_PATH / \"output\" / \"dl_training\"\n",
    "\n",
    "BATCH_SIZE = 32\n",
    "NUM_WORKERS = 0  # Windows compatibility\n",
    "NUM_EPOCHS = 30\n",
    "NUM_FOLDS = 5\n",
    "\n",
    "USE_AMP = True\n",
    "PIN_MEMORY = True\n",
    "DROP_RATE = 0.2  # Reduced further for lightweight models\n",
    "\n",
    "EARLY_STOPPING_PATIENCE = 5\n",
    "GRAD_CLIP_VALUE = 1.0\n",
    "\n",
    "# Updated model definitions - Focusing on lightweight, high-performance models\n",
    "MODELS = {\n",
    "    # Very lightweight models\n",
    "    'mobilenetv3_small_100': {\n",
    "        'pretrained': True,\n",
    "        'lr': 5e-5,\n",
    "        'description': 'Ultra lightweight MobileNetV3 Small variant'\n",
    "    },\n",
    "    'efficientnet_lite0': {\n",
    "        'pretrained': True,\n",
    "        'lr': 5e-5,\n",
    "        'description': 'Lightweight EfficientNet variant optimized for mobile'\n",
    "    },\n",
    "    'mobilevit_xxs': {\n",
    "        'pretrained': True,\n",
    "        'lr': 5e-5,\n",
    "        'description': 'Extra small MobileViT - combines mobility and transformer benefits'\n",
    "    },\n",
    "    'shufflenet_v2_x1_0': {\n",
    "        'pretrained': True,\n",
    "        'lr': 5e-5,\n",
    "        'description': 'ShuffleNetV2 with 1.0x output channels - very efficient'\n",
    "    },\n",
    "    # Slightly larger but still efficient models\n",
    "    'efficientnet_b0': {\n",
    "        'pretrained': True,\n",
    "        'lr': 5e-5,\n",
    "        'description': 'Smallest EfficientNet variant with good accuracy-size tradeoff'\n",
    "    },\n",
    "    'mobilevit_xs': {\n",
    "        'pretrained': True,\n",
    "        'lr': 5e-5,\n",
    "        'description': 'Extra small MobileViT - larger than xxs but still efficient'\n",
    "    }\n",
    "}\n",
    "\n",
    "# Device Setup\n",
    "if torch.cuda.is_available():\n",
    "    DEVICE = torch.device(\"cuda\")\n",
    "    logger.info(f\"CUDA is available! Using {torch.cuda.get_device_name(0)}\")\n",
    "    logger.info(f\"CUDA Version: {torch.version.cuda}\")\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "    torch.backends.cuda.matmul.allow_tf32 = True\n",
    "    torch.backends.cudnn.allow_tf32 = True\n",
    "else:\n",
    "    DEVICE = torch.device(\"cpu\")\n",
    "    logger.info(\"CUDA is not available. Using CPU.\")\n",
    "\n",
    "# Set random seed\n",
    "def set_random_seed(seed=42):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "    cudnn.deterministic = True\n",
    "    cudnn.benchmark = True\n",
    "\n",
    "set_random_seed(42)\n",
    "\n",
    "# Create output directories if necessary\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "os.makedirs(OUTPUT_DIR / 'logs', exist_ok=True)\n",
    "os.makedirs(OUTPUT_DIR / 'models', exist_ok=True)\n",
    "os.makedirs(OUTPUT_DIR / 'results', exist_ok=True)\n",
    "os.makedirs(OUTPUT_DIR / 'visualizations', exist_ok=True)\n",
    "\n",
    "# Enhanced CoffeeDataset class to handle the specified directory structure\n",
    "class CoffeeDataset(Dataset):\n",
    "    \"\"\"Dataset class for loading coffee bean images with enhanced augmentation strategies.\"\"\"\n",
    "    def __init__(self, root_dir, split='train', test_ratio=0.15, val_ratio=0.15):\n",
    "        self.root_dir = root_dir\n",
    "        self.split = split\n",
    "        self.logger = logging.getLogger('CoffeeDataset')\n",
    "        self.samples = []\n",
    "        self.max_retries = 3\n",
    "\n",
    "        # Load all samples from the specified directory structure\n",
    "        normal_path = os.path.join(self.root_dir, \"Normal\")\n",
    "        defect_path = os.path.join(self.root_dir, \"Defect\")\n",
    "\n",
    "        # Handle case sensitivity\n",
    "        if not os.path.exists(normal_path):\n",
    "            normal_path = os.path.join(self.root_dir, \"normal\")\n",
    "        if not os.path.exists(defect_path):\n",
    "            defect_path = os.path.join(self.root_dir, \"defect\")\n",
    "\n",
    "        self.logger.info(f\"Loading samples from Normal: {normal_path}\")\n",
    "        self.logger.info(f\"Loading samples from Defect: {defect_path}\")\n",
    "\n",
    "        # Process methods (Dry, Honey, Wet)\n",
    "        all_filepaths = []\n",
    "        all_labels = []\n",
    "\n",
    "        # Process Normal samples\n",
    "        for method in ['Dry', 'Honey', 'Wet', 'dry', 'honey', 'wet']:\n",
    "            method_path = os.path.join(normal_path, method)\n",
    "            if not os.path.exists(method_path):\n",
    "                continue\n",
    "\n",
    "            for roast in ['Dark', 'Light', 'Medium', 'dark', 'light', 'medium']:\n",
    "                roast_path = os.path.join(method_path, roast)\n",
    "                if not os.path.exists(roast_path):\n",
    "                    continue\n",
    "\n",
    "                for img_name in os.listdir(roast_path):\n",
    "                    if img_name.lower().endswith(('.png', '.jpg', '.jpeg')):\n",
    "                        all_filepaths.append(os.path.join(roast_path, img_name))\n",
    "                        all_labels.append(0)  # Normal\n",
    "\n",
    "        # Process Defect samples\n",
    "        for method in ['Dry', 'Honey', 'Wet', 'dry', 'honey', 'wet']:\n",
    "            method_path = os.path.join(defect_path, method)\n",
    "            if not os.path.exists(method_path):\n",
    "                continue\n",
    "\n",
    "            for roast in ['Dark', 'Light', 'Medium', 'dark', 'light', 'medium']:\n",
    "                roast_path = os.path.join(method_path, roast)\n",
    "                if not os.path.exists(roast_path):\n",
    "                    continue\n",
    "\n",
    "                for img_name in os.listdir(roast_path):\n",
    "                    if img_name.lower().endswith(('.png', '.jpg', '.jpeg')):\n",
    "                        all_filepaths.append(os.path.join(roast_path, img_name))\n",
    "                        all_labels.append(1)  # Defect\n",
    "\n",
    "        # Split the dataset into train, validation, and test sets\n",
    "        unique_paths = list(set(all_filepaths))  # Ensure no duplicates\n",
    "        random.shuffle(unique_paths)\n",
    "\n",
    "        test_size = int(len(unique_paths) * test_ratio)\n",
    "        val_size = int(len(unique_paths) * val_ratio)\n",
    "\n",
    "        test_paths = set(unique_paths[:test_size])\n",
    "        val_paths = set(unique_paths[test_size:test_size + val_size])\n",
    "        train_paths = set(unique_paths[test_size + val_size:])\n",
    "\n",
    "        # Assign samples based on split\n",
    "        if split == 'train':\n",
    "            target_paths = train_paths\n",
    "        elif split == 'val':\n",
    "            target_paths = val_paths\n",
    "        else:  # 'test'\n",
    "            target_paths = test_paths\n",
    "\n",
    "        # Filter samples for the current split\n",
    "        for filepath, label in zip(all_filepaths, all_labels):\n",
    "            if filepath in target_paths:\n",
    "                self.samples.append({\n",
    "                    'filepath': filepath,\n",
    "                    'label': label\n",
    "                })\n",
    "\n",
    "        self.logger.info(f\"Loaded {len(self.samples)} samples for split: {self.split}\")\n",
    "\n",
    "        # Analyze class distribution\n",
    "        labels = [sample['label'] for sample in self.samples]\n",
    "        class_counts = Counter(labels)\n",
    "        total = len(labels)\n",
    "\n",
    "        self.logger.info(f\"\\n{self.split} set class distribution:\")\n",
    "        for label, count in class_counts.items():\n",
    "            percentage = (count / total) * 100\n",
    "            self.logger.info(f\"Class {label}: {count} samples ({percentage:.2f}%)\")\n",
    "\n",
    "        # Enhanced but lightweight data augmentation for training (optimized for speed)\n",
    "        if self.split == 'train':\n",
    "            self.transform = transforms.Compose([\n",
    "                transforms.RandomResizedCrop(224, scale=(0.8, 1.0)),\n",
    "                transforms.RandomHorizontalFlip(p=0.5),\n",
    "                transforms.RandomVerticalFlip(p=0.3),\n",
    "                transforms.RandomRotation(\n",
    "                    degrees=30,\n",
    "                    fill=1.0,\n",
    "                    interpolation=transforms.InterpolationMode.BILINEAR\n",
    "                ),\n",
    "                transforms.ColorJitter(\n",
    "                    brightness=0.2, contrast=0.2,\n",
    "                    saturation=0.2, hue=0.1\n",
    "                ),\n",
    "                transforms.ToTensor(),\n",
    "                transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "            ])\n",
    "        else:\n",
    "            # Validation and test transforms - keep simple for efficiency\n",
    "            self.transform = transforms.Compose([\n",
    "                transforms.Resize(256),\n",
    "                transforms.CenterCrop(224),\n",
    "                transforms.ToTensor(),\n",
    "                transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "            ])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        sample = self.samples[idx]\n",
    "        image_path = sample['filepath']\n",
    "        label = sample['label']\n",
    "\n",
    "        for attempt in range(self.max_retries):\n",
    "            try:\n",
    "                image = Image.open(image_path).convert('RGB')\n",
    "                return self.transform(image), label\n",
    "            except Exception as e:\n",
    "                self.logger.warning(\n",
    "                    f\"Failed to load/transform image {image_path} on attempt {attempt+1}: {str(e)}\"\n",
    "                )\n",
    "                idx = (idx + 1) % len(self)\n",
    "\n",
    "        self.logger.error(\n",
    "            f\"Failed to load image {image_path} after {self.max_retries} attempts. Returning default sample.\"\n",
    "        )\n",
    "        return torch.zeros(3, 224, 224), -1\n",
    "\n",
    "# Create Datasets\n",
    "logger.info(\"Creating train/val/test datasets...\")\n",
    "\n",
    "# Create separate instances for train, val, test\n",
    "train_dataset = CoffeeDataset(str(DATA_DIR), split='train')\n",
    "val_dataset = CoffeeDataset(str(DATA_DIR), split='val')\n",
    "test_dataset = CoffeeDataset(str(DATA_DIR), split='test')\n",
    "\n",
    "# Create Data Loaders\n",
    "train_loader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=True,\n",
    "    num_workers=NUM_WORKERS,\n",
    "    pin_memory=PIN_MEMORY,\n",
    "    drop_last=True\n",
    ")\n",
    "\n",
    "val_loader = DataLoader(\n",
    "    val_dataset,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=False,\n",
    "    num_workers=NUM_WORKERS,\n",
    "    pin_memory=PIN_MEMORY\n",
    ")\n",
    "\n",
    "test_loader = DataLoader(\n",
    "    test_dataset,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=False,\n",
    "    num_workers=NUM_WORKERS,\n",
    "    pin_memory=PIN_MEMORY\n",
    ")\n",
    "\n",
    "logger.info(\"Dataset sizes:\")\n",
    "logger.info(f\"Train: {len(train_dataset)} samples\")\n",
    "logger.info(f\"Validation: {len(val_dataset)} samples\")\n",
    "logger.info(f\"Test: {len(test_dataset)} samples\")\n",
    "\n",
    "# Early stopping parameters\n",
    "early_stopping_patience = EARLY_STOPPING_PATIENCE\n",
    "early_stopping_min_delta = 0.001\n",
    "\n",
    "# Metric calculation function\n",
    "def calculate_metrics_inline(targets, predictions, probabilities=None):\n",
    "    \"\"\"Calculate metrics such as accuracy, precision, recall, f1-score, specificity, confusion matrix.\"\"\"\n",
    "    try:\n",
    "        if len(targets) != len(predictions):\n",
    "            raise ValueError(\"Length of targets and predictions must match\")\n",
    "        if probabilities is not None and len(targets) != len(probabilities):\n",
    "            raise ValueError(\"Length of targets and probabilities must match\")\n",
    "\n",
    "        metrics = {\n",
    "            'accuracy': accuracy_score(targets, predictions),\n",
    "            'precision': precision_score(targets, predictions, average='binary', zero_division=0),\n",
    "            'recall': recall_score(targets, predictions, average='binary', zero_division=0),\n",
    "            'f1_score': f1_score(targets, predictions, average='binary', zero_division=0)\n",
    "        }\n",
    "\n",
    "        if probabilities is not None:\n",
    "            try:\n",
    "                metrics['roc_auc'] = roc_auc_score(targets, probabilities)\n",
    "            except ValueError as e:\n",
    "                logger.warning(f\"ROC-AUC calculation failed: {str(e)}\")\n",
    "                metrics['roc_auc'] = 0.0\n",
    "\n",
    "        cm = confusion_matrix(targets, predictions)\n",
    "        if cm.shape == (2, 2):\n",
    "            tn, fp, fn, tp = cm.ravel()\n",
    "            metrics.update({\n",
    "                'specificity': tn / (tn + fp) if (tn + fp) > 0 else 0,\n",
    "                'confusion_matrix': cm.tolist()\n",
    "            })\n",
    "\n",
    "        return metrics\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error in metrics calculation: {str(e)}\")\n",
    "        return {\n",
    "            'accuracy': 0.0,\n",
    "            'precision': 0.0,\n",
    "            'recall': 0.0,\n",
    "            'f1_score': 0.0,\n",
    "            'confusion_matrix': [[0, 0], [0, 0]],\n",
    "            'specificity': 0.0,\n",
    "            'error': str(e)\n",
    "        }\n",
    "\n",
    "# Enhanced model metrics calculation\n",
    "def compute_model_metrics_inline(model):\n",
    "    \"\"\"Calculate model size, params count, and complexity metrics.\"\"\"\n",
    "    # Calculate parameter count\n",
    "    param_count = sum(p.numel() for p in model.parameters())\n",
    "\n",
    "    # Calculate model size in MB\n",
    "    param_size = 0\n",
    "    for param in model.parameters():\n",
    "        param_size += param.nelement() * param.element_size()\n",
    "    buffer_size = 0\n",
    "    for buffer in model.buffers():\n",
    "        buffer_size += buffer.nelement() * buffer.element_size()\n",
    "    size_mb = (param_size + buffer_size) / 1024**2\n",
    "\n",
    "    return {\n",
    "        'param_count': param_count,\n",
    "        'size_mb': size_mb\n",
    "    }\n",
    "\n",
    "# Plot confusion matrix\n",
    "def plot_confusion_matrix_inline(cm, output_dir, model_name):\n",
    "    \"\"\"Plot confusion matrix.\"\"\"\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=['Normal', 'Defect'], yticklabels=['Normal', 'Defect'])\n",
    "    plt.title(f'Confusion Matrix - {model_name}')\n",
    "    plt.ylabel('True Label')\n",
    "    plt.xlabel('Predicted Label')\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    plt.savefig(os.path.join(output_dir, f'{model_name}_confusion_matrix.png'))\n",
    "    plt.close()\n",
    "\n",
    "# Plot training curves\n",
    "def plot_training_curves_inline(history, output_dir, model_name):\n",
    "    \"\"\"Plot training loss, accuracy, LR schedule, etc.\"\"\"\n",
    "    sns.set_theme(style='whitegrid')\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "    fig.suptitle(f'Training Curves - {model_name}', fontsize=16)\n",
    "\n",
    "    axes[0, 0].plot(history['train_loss'], label='Training Loss')\n",
    "    axes[0, 0].plot(history['val_loss'], label='Validation Loss')\n",
    "    axes[0, 0].set_title('Loss Curves')\n",
    "    axes[0, 0].set_xlabel('Epoch')\n",
    "    axes[0, 0].set_ylabel('Loss')\n",
    "    axes[0, 0].legend()\n",
    "\n",
    "    axes[0, 1].plot(history['train_acc'], label='Training Accuracy')\n",
    "    axes[0, 1].plot(history['val_acc'], label='Validation Accuracy')\n",
    "    axes[0, 1].set_title('Accuracy Curves')\n",
    "    axes[0, 1].set_xlabel('Epoch')\n",
    "    axes[0, 1].set_ylabel('Accuracy')\n",
    "    axes[0, 1].legend()\n",
    "\n",
    "    axes[1, 0].plot(history['lr_history'], label='Learning Rate')\n",
    "    axes[1, 0].set_title('Learning Rate Schedule')\n",
    "    axes[1, 0].set_xlabel('Epoch')\n",
    "    axes[1, 0].set_ylabel('Learning Rate')\n",
    "    axes[1, 0].set_yscale('log')\n",
    "\n",
    "    axes[1, 1].plot(history['epoch_times'], label='Training Time')\n",
    "    axes[1, 1].set_title('Training Time per Epoch')\n",
    "    axes[1, 1].set_xlabel('Epoch')\n",
    "    axes[1, 1].set_ylabel('Time (seconds)')\n",
    "\n",
    "    plt.tight_layout(rect=[0, 0.03, 1, 0.95])\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    plt.savefig(os.path.join(output_dir, f'{model_name}_training_curves.png'))\n",
    "    plt.close()\n",
    "\n",
    "# Enhanced model evaluation function with detailed timing\n",
    "def evaluate_model_inline(model, test_loader, criterion):\n",
    "    model.eval()\n",
    "    running_loss = 0.0\n",
    "    predictions = []\n",
    "    targets = []\n",
    "    probabilities = []\n",
    "    inference_times = []\n",
    "    batch_sizes = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for images, labels in test_loader:\n",
    "            batch_sizes.append(images.shape[0])\n",
    "            images = images.to(DEVICE, non_blocking=True)\n",
    "            labels = labels.to(DEVICE, non_blocking=True)\n",
    "\n",
    "            # Warm-up run to eliminate first-batch overhead\n",
    "            if len(inference_times) == 0:\n",
    "                _ = model(images)\n",
    "                torch.cuda.synchronize()\n",
    "\n",
    "            # Measure inference time with synchronization for accuracy\n",
    "            torch.cuda.synchronize()\n",
    "            start_time = time.time()\n",
    "            outputs = model(images)\n",
    "            torch.cuda.synchronize()\n",
    "            inference_time = time.time() - start_time\n",
    "            inference_times.append(inference_time)\n",
    "\n",
    "            loss = criterion(outputs, labels)\n",
    "            probs = torch.softmax(outputs, dim=1)[:, 1]\n",
    "\n",
    "            running_loss += loss.item() * images.size(0)\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "\n",
    "            predictions.extend(preds.cpu().numpy())\n",
    "            targets.extend(labels.cpu().numpy())\n",
    "            probabilities.extend(probs.cpu().numpy())\n",
    "\n",
    "    # Compute metrics\n",
    "    test_loss = running_loss / len(test_loader.dataset)\n",
    "    metrics = calculate_metrics_inline(targets, predictions, probabilities)\n",
    "    metrics['loss'] = test_loss\n",
    "\n",
    "    # More accurate inference time calculations\n",
    "    total_samples = sum(batch_sizes)\n",
    "    total_inference_time = sum(inference_times)\n",
    "\n",
    "    # Skip the first batch timing (warm-up)\n",
    "    if len(inference_times) > 1:\n",
    "        avg_inference_time = sum(inference_times[1:]) / (len(inference_times) - 1)\n",
    "        inference_time_per_image = sum(inference_times[1:]) / sum(batch_sizes[1:])\n",
    "    else:\n",
    "        avg_inference_time = total_inference_time\n",
    "        inference_time_per_image = total_inference_time / total_samples\n",
    "\n",
    "    metrics['avg_inference_time'] = avg_inference_time  # Average time per batch\n",
    "    metrics['inference_time_per_image'] = inference_time_per_image  # Average time per image\n",
    "    metrics['total_inference_time'] = total_inference_time\n",
    "    metrics['images_per_second'] = 1.0 / inference_time_per_image if inference_time_per_image > 0 else 0\n",
    "\n",
    "    return metrics, targets, predictions, probabilities\n",
    "\n",
    "# Start model evaluation and selection\n",
    "logger.info(\"Initializing models for evaluation...\")\n",
    "\n",
    "models_info = {}\n",
    "\n",
    "# Initialize and evaluate each model\n",
    "for model_name, model_cfg in MODELS.items():\n",
    "    try:\n",
    "        start_time = time.time()\n",
    "\n",
    "        # Create the model\n",
    "        model = timm.create_model(\n",
    "            model_name,\n",
    "            pretrained=model_cfg['pretrained'],\n",
    "            num_classes=2,\n",
    "            drop_rate=DROP_RATE\n",
    "        ).to(DEVICE)\n",
    "\n",
    "        # Calculate model metrics\n",
    "        model_metrics = compute_model_metrics_inline(model)\n",
    "\n",
    "        # Store model information\n",
    "        models_info[model_name] = {\n",
    "            'model': model,\n",
    "            'lr': model_cfg['lr'],\n",
    "            'size_mb': model_metrics['size_mb'],\n",
    "            'param_count': model_metrics['param_count'],\n",
    "            'description': model_cfg.get('description', 'No description provided')\n",
    "        }\n",
    "\n",
    "        logger.info(f\"Initialized model: {model_name}\")\n",
    "        logger.info(f\"  - Parameters: {model_metrics['param_count']:,}\")\n",
    "        logger.info(f\"  - Size: {model_metrics['size_mb']:.2f} MB\")\n",
    "        logger.info(f\"  - Description: {model_cfg.get('description', 'Not provided')}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Failed to initialize model {model_name}: {str(e)}\")\n",
    "\n",
    "# Single fold validation to quickly assess each model\n",
    "logger.info(\"\\nPerforming quick validation of all models...\")\n",
    "\n",
    "validation_results = {}\n",
    "\n",
    "for model_name, m_info in models_info.items():\n",
    "    logger.info(f\"\\nValidating model: {model_name}\")\n",
    "\n",
    "    # Set up optimizer and criterion\n",
    "    model = m_info['model']\n",
    "    optimizer = torch.optim.AdamW(\n",
    "        model.parameters(),\n",
    "        lr=m_info['lr'],\n",
    "        weight_decay=0.01,\n",
    "        betas=(0.9, 0.999)\n",
    "    )\n",
    "\n",
    "    scheduler = CosineAnnealingLR(\n",
    "        optimizer,\n",
    "        T_max=10,  # Shorter for quick validation\n",
    "        eta_min=1e-6\n",
    "    )\n",
    "\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    scaler = GradScaler(enabled=USE_AMP)\n",
    "\n",
    "    # Train for just a few epochs to get a sense of model performance\n",
    "    train_time_start = time.time()\n",
    "\n",
    "    for epoch in range(5):  # Train for just 5 epochs for quick assessment\n",
    "        # Train step\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "\n",
    "        for images, labels in train_loader:\n",
    "            images = images.to(DEVICE, non_blocking=True)\n",
    "            labels = labels.to(DEVICE, non_blocking=True)\n",
    "            optimizer.zero_grad(set_to_none=True)\n",
    "\n",
    "            with autocast(device_type='cuda', enabled=USE_AMP):\n",
    "                outputs = model(images)\n",
    "                loss = criterion(outputs, labels)\n",
    "\n",
    "            if USE_AMP:\n",
    "                scaler.scale(loss).backward()\n",
    "                scaler.unscale_(optimizer)\n",
    "                torch.nn.utils.clip_grad_norm_(model.parameters(), GRAD_CLIP_VALUE)\n",
    "                scaler.step(optimizer)\n",
    "                scaler.update()\n",
    "            else:\n",
    "                loss.backward()\n",
    "                torch.nn.utils.clip_grad_norm_(model.parameters(), GRAD_CLIP_VALUE)\n",
    "                optimizer.step()\n",
    "\n",
    "            running_loss += loss.item() * images.size(0)\n",
    "\n",
    "        train_loss = running_loss / len(train_loader.dataset)\n",
    "        scheduler.step()\n",
    "\n",
    "        # Log progress for each epoch\n",
    "        logger.info(f\"  Epoch {epoch+1}/5 - Train Loss: {train_loss:.4f}\")\n",
    "\n",
    "    train_time = time.time() - train_time_start\n",
    "\n",
    "    # Evaluate on validation set\n",
    "    logger.info(f\"Evaluating {model_name} on validation set...\")\n",
    "    val_metrics, val_targets, val_preds, val_probs = evaluate_model_inline(\n",
    "        model, val_loader, criterion\n",
    "    )\n",
    "\n",
    "    # Evaluate on test set\n",
    "    logger.info(f\"Evaluating {model_name} on test set...\")\n",
    "    test_metrics, test_targets, test_preds, test_probs = evaluate_model_inline(\n",
    "        model, test_loader, criterion\n",
    "    )\n",
    "\n",
    "    # Store results\n",
    "    validation_results[model_name] = {\n",
    "        'val_metrics': val_metrics,\n",
    "        'test_metrics': test_metrics,\n",
    "        'train_time_total': train_time,\n",
    "        'train_time_per_epoch': train_time / 5,\n",
    "        'size_mb': m_info['size_mb'],\n",
    "        'param_count': m_info['param_count']\n",
    "    }\n",
    "\n",
    "    # Log model performance\n",
    "    logger.info(f\"\\nPerformance Summary for {model_name}:\")\n",
    "    logger.info(f\"  Model Size: {m_info['size_mb']:.2f} MB\")\n",
    "    logger.info(f\"  Parameters: {m_info['param_count']:,}\")\n",
    "    logger.info(f\"  Training Time: {train_time:.2f}s ({train_time/5:.2f}s per epoch)\")\n",
    "    logger.info(f\"  Inference Speed: {test_metrics['images_per_second']:.2f} images/second\")\n",
    "    logger.info(f\"  Validation Accuracy: {val_metrics['accuracy']:.4f}\")\n",
    "    logger.info(f\"  Validation F1: {val_metrics['f1_score']:.4f}\")\n",
    "    logger.info(f\"  Test Accuracy: {test_metrics['accuracy']:.4f}\")\n",
    "    logger.info(f\"  Test F1: {test_metrics['f1_score']:.4f}\")\n",
    "\n",
    "    # Save model state for top models\n",
    "    model_save_path = os.path.join(str(OUTPUT_DIR), 'models', f\"{model_name}_quick_val.pth\")\n",
    "    torch.save(model.state_dict(), model_save_path)\n",
    "\n",
    "# Choose best models based on validation results\n",
    "logger.info(\"\\nSelecting best models for full training...\")\n",
    "\n",
    "# Define a balanced score that considers accuracy, speed, and model size\n",
    "def calculate_model_efficiency_score(result):\n",
    "    accuracy = result['test_metrics']['accuracy']\n",
    "    f1_score = result['test_metrics']['f1_score']\n",
    "    inference_speed = result['test_metrics']['images_per_second']\n",
    "    model_size = result['size_mb']\n",
    "\n",
    "    # Balanced score formula - higher is better\n",
    "    # 50% for performance (F1 + accuracy), 30% for inference speed, 20% for model size efficiency\n",
    "    performance_score = 0.3 * accuracy + 0.2 * f1_score\n",
    "    speed_score = 0.3 * min(1.0, inference_speed / 100)  # Normalize speed, cap at 100 img/sec\n",
    "    size_score = 0.2 * (1.0 - min(1.0, model_size / 100))  # Smaller size is better, cap at 100MB\n",
    "\n",
    "    return performance_score + speed_score + size_score\n",
    "\n",
    "# Calculate efficiency scores\n",
    "model_scores = {}\n",
    "for model_name, result in validation_results.items():\n",
    "    score = calculate_model_efficiency_score(result)\n",
    "    model_scores[model_name] = score\n",
    "\n",
    "# Sort models by score\n",
    "sorted_models = sorted(model_scores.items(), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "# Select top 3 models for full training\n",
    "top_models = [name for name, _ in sorted_models[:3]]\n",
    "\n",
    "logger.info(\"Selected models for full training:\")\n",
    "for i, model_name in enumerate(top_models):\n",
    "    result = validation_results[model_name]\n",
    "    logger.info(f\"{i+1}. {model_name}\")\n",
    "    logger.info(f\"   - Score: {model_scores[model_name]:.4f}\")\n",
    "    logger.info(f\"   - Size: {result['size_mb']:.2f} MB\")\n",
    "    logger.info(f\"   - F1 Score: {result['test_metrics']['f1_score']:.4f}\")\n",
    "    logger.info(f\"   - Inference: {result['test_metrics']['images_per_second']:.2f} img/sec\")\n",
    "\n",
    "# Full training with K-Fold cross-validation for selected models\n",
    "logger.info(\"\\nStarting full K-Fold cross-validation for selected models...\")\n",
    "\n",
    "# Training function for full training\n",
    "def train_model_with_kfold(model_name, model_info):\n",
    "    logger.info(f\"\\nStarting full training for model: {model_name}\")\n",
    "    start_time = time.time()\n",
    "    current_fold_metrics = []\n",
    "    best_global_state = None\n",
    "    best_global_loss = float('inf')\n",
    "    best_global_fold = None\n",
    "\n",
    "    # Prepare K-Fold splits\n",
    "    labels_train_dataset = [sample['label'] for sample in train_dataset.samples]\n",
    "    kf = StratifiedKFold(n_splits=NUM_FOLDS, shuffle=True, random_state=42)\n",
    "\n",
    "    for fold, (train_indices, val_indices) in enumerate(kf.split(train_dataset.samples, labels_train_dataset), 1):\n",
    "        logger.info(f\"\\nStarting fold {fold}/{NUM_FOLDS}\")\n",
    "\n",
    "        # Create subset for train/val\n",
    "        train_subset = torch.utils.data.Subset(train_dataset, train_indices)\n",
    "        val_subset = torch.utils.data.Subset(train_dataset, val_indices)\n",
    "\n",
    "        # Re-init model for each fold\n",
    "        model = timm.create_model(\n",
    "            model_name,\n",
    "            pretrained=True,\n",
    "            num_classes=2,\n",
    "            drop_rate=DROP_RATE\n",
    "        ).to(DEVICE)\n",
    "\n",
    "        # Set up optimizer\n",
    "        optimizer = torch.optim.AdamW(\n",
    "            model.parameters(),\n",
    "            lr=model_info['lr'],\n",
    "            weight_decay=0.01,\n",
    "            betas=(0.9, 0.999)\n",
    "        )\n",
    "\n",
    "        # Set up learning rate scheduler\n",
    "        scheduler = CosineAnnealingLR(\n",
    "            optimizer,\n",
    "            T_max=NUM_EPOCHS,\n",
    "            eta_min=1e-6\n",
    "        )\n",
    "\n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "        scaler = GradScaler(enabled=USE_AMP)\n",
    "\n",
    "        # Create DataLoader for subset\n",
    "        fold_train_loader = DataLoader(\n",
    "            train_subset,\n",
    "            batch_size=BATCH_SIZE,\n",
    "            shuffle=True,\n",
    "            num_workers=NUM_WORKERS,\n",
    "            pin_memory=PIN_MEMORY,\n",
    "            drop_last=True\n",
    "        )\n",
    "\n",
    "        fold_val_loader = DataLoader(\n",
    "            val_subset,\n",
    "            batch_size=BATCH_SIZE,\n",
    "            shuffle=False,\n",
    "            num_workers=NUM_WORKERS,\n",
    "            pin_memory=PIN_MEMORY\n",
    "        )\n",
    "\n",
    "        # Prepare training history\n",
    "        training_history = {\n",
    "            'train_loss': [], 'val_loss': [],\n",
    "            'train_acc': [], 'val_acc': [],\n",
    "            'train_metrics': [], 'val_metrics': [],\n",
    "            'lr_history': [], 'epoch_times': []\n",
    "        }\n",
    "\n",
    "        best_val_loss = float('inf')\n",
    "        best_model_state = None\n",
    "\n",
    "        # Early stopping state\n",
    "        es_best_loss = None\n",
    "        es_counter = 0\n",
    "        es_early_stop = False\n",
    "\n",
    "        # Start epoch loop\n",
    "        for epoch in range(NUM_EPOCHS):\n",
    "            epoch_start_time = time.time()\n",
    "\n",
    "            # -------------------- TRAIN EPOCH --------------------\n",
    "            model.train()\n",
    "            running_loss = 0.0\n",
    "            predictions = []\n",
    "            targets_list = []\n",
    "            probabilities_list = []\n",
    "\n",
    "            for images, labels in fold_train_loader:\n",
    "                images = images.to(DEVICE, non_blocking=True)\n",
    "                labels = labels.to(DEVICE, non_blocking=True)\n",
    "                optimizer.zero_grad(set_to_none=True)\n",
    "\n",
    "                with autocast(device_type='cuda', enabled=USE_AMP):\n",
    "                    outputs = model(images)\n",
    "                    loss = criterion(outputs, labels)\n",
    "                    probs = torch.softmax(outputs, dim=1)[:, 1]\n",
    "\n",
    "                if USE_AMP:\n",
    "                    scaler.scale(loss).backward()\n",
    "                    scaler.unscale_(optimizer)\n",
    "                    torch.nn.utils.clip_grad_norm_(model.parameters(), GRAD_CLIP_VALUE)\n",
    "                    scaler.step(optimizer)\n",
    "                    scaler.update()\n",
    "                else:\n",
    "                    loss.backward()\n",
    "                    torch.nn.utils.clip_grad_norm_(model.parameters(), GRAD_CLIP_VALUE)\n",
    "                    optimizer.step()\n",
    "\n",
    "                running_loss += loss.item() * images.size(0)\n",
    "                _, preds = torch.max(outputs, 1)\n",
    "                predictions.extend(preds.detach().cpu().numpy())\n",
    "                probabilities_list.extend(probs.detach().cpu().numpy())\n",
    "                targets_list.extend(labels.cpu().numpy())\n",
    "\n",
    "            epoch_train_loss = running_loss / len(fold_train_loader.dataset)\n",
    "            train_metrics_dict = calculate_metrics_inline(targets_list, predictions, probabilities_list)\n",
    "            train_metrics_dict['loss'] = epoch_train_loss\n",
    "\n",
    "            # -------------------- VALIDATE EPOCH --------------------\n",
    "            model.eval()\n",
    "            val_running_loss = 0.0\n",
    "            val_predictions = []\n",
    "            val_targets_list = []\n",
    "            val_probabilities_list = []\n",
    "\n",
    "            with torch.no_grad():\n",
    "                for images_val, labels_val in fold_val_loader:\n",
    "                    images_val = images_val.to(DEVICE, non_blocking=True)\n",
    "                    labels_val = labels_val.to(DEVICE, non_blocking=True)\n",
    "                    outputs_val = model(images_val)\n",
    "                    loss_val = criterion(outputs_val, labels_val)\n",
    "                    probs_val = torch.softmax(outputs_val, dim=1)[:, 1]\n",
    "\n",
    "                    val_running_loss += loss_val.item() * images_val.size(0)\n",
    "                    _, preds_val = torch.max(outputs_val, 1)\n",
    "                    val_predictions.extend(preds_val.detach().cpu().numpy())\n",
    "                    val_probabilities_list.extend(probs_val.detach().cpu().numpy())\n",
    "                    val_targets_list.extend(labels_val.cpu().numpy())\n",
    "\n",
    "            val_loss = val_running_loss / len(fold_val_loader.dataset)\n",
    "            val_metrics_dict = calculate_metrics_inline(val_targets_list, val_predictions, val_probabilities_list)\n",
    "            val_metrics_dict['loss'] = val_loss\n",
    "\n",
    "            # Logging training/val info\n",
    "            current_lr = optimizer.param_groups[0]['lr']\n",
    "            logger.info(\n",
    "                f\"Epoch {epoch+1}/{NUM_EPOCHS} | \"\n",
    "                f\"Train Loss: {train_metrics_dict['loss']:.4f} | \"\n",
    "                f\"Val Loss: {val_metrics_dict['loss']:.4f} | \"\n",
    "                f\"Val F1: {val_metrics_dict['f1_score']:.4f} | \"\n",
    "                f\"Val Acc: {val_metrics_dict['accuracy']:.4f}\"\n",
    "            )\n",
    "\n",
    "            # Step the scheduler\n",
    "            scheduler.step()\n",
    "\n",
    "            # Update training history\n",
    "            training_history['train_loss'].append(train_metrics_dict['loss'])\n",
    "            training_history['val_loss'].append(val_metrics_dict['loss'])\n",
    "            training_history['train_acc'].append(train_metrics_dict['accuracy'])\n",
    "            training_history['val_acc'].append(val_metrics_dict['accuracy'])\n",
    "            training_history['train_metrics'].append(train_metrics_dict)\n",
    "            training_history['val_metrics'].append(val_metrics_dict)\n",
    "            training_history['lr_history'].append(current_lr)\n",
    "            training_history['epoch_times'].append(time.time() - epoch_start_time)\n",
    "\n",
    "            # Save best model for this fold\n",
    "            if val_loss < best_val_loss:\n",
    "                best_val_loss = val_loss\n",
    "                best_model_state = model.state_dict().copy()\n",
    "\n",
    "            # Inline early stopping\n",
    "            if es_best_loss is None:\n",
    "                es_best_loss = val_loss\n",
    "            else:\n",
    "                if val_loss > es_best_loss - early_stopping_min_delta:\n",
    "                    es_counter += 1\n",
    "                    if es_counter >= early_stopping_patience:\n",
    "                        es_early_stop = True\n",
    "                else:\n",
    "                    es_best_loss = val_loss\n",
    "                    es_counter = 0\n",
    "\n",
    "            if es_early_stop:\n",
    "                logger.info(\"Early stopping triggered.\")\n",
    "                break\n",
    "\n",
    "        # After finishing epochs for this fold\n",
    "        if best_model_state is not None:\n",
    "            model.load_state_dict(best_model_state)\n",
    "\n",
    "        # Store fold results\n",
    "        fold_results = {\n",
    "            'model_state': best_model_state,\n",
    "            'metrics': training_history,\n",
    "            'best_val_loss': best_val_loss\n",
    "        }\n",
    "\n",
    "        # Summarize fold results\n",
    "        final_train_metrics = fold_results['metrics']['train_metrics'][-1]\n",
    "        final_val_metrics = fold_results['metrics']['val_metrics'][-1]\n",
    "\n",
    "        summary_str = (\n",
    "            f\"\\nFold {fold} Summary - {model_name}:\\n\"\n",
    "            f\"Validation Metrics:\\n\"\n",
    "            f\"- Accuracy: {final_val_metrics['accuracy']:.4f}\\n\"\n",
    "            f\"- F1 Score: {final_val_metrics['f1_score']:.4f}\\n\"\n",
    "            f\"- Precision: {final_val_metrics['precision']:.4f}\\n\"\n",
    "            f\"- Recall: {final_val_metrics['recall']:.4f}\\n\"\n",
    "        )\n",
    "        logger.info(summary_str)\n",
    "\n",
    "        current_fold_metrics.append(fold_results['metrics'])\n",
    "\n",
    "        # Check if this fold is better globally\n",
    "        if best_val_loss < best_global_loss:\n",
    "            best_global_loss = best_val_loss\n",
    "            best_global_state = best_model_state\n",
    "            best_global_fold = fold\n",
    "\n",
    "        # Clean up\n",
    "        del train_subset, val_subset, fold_train_loader, fold_val_loader, optimizer, criterion, model\n",
    "        torch.cuda.empty_cache()\n",
    "        gc.collect()\n",
    "\n",
    "    # After all folds, recreate the model and load best weights\n",
    "    best_model = timm.create_model(\n",
    "        model_name,\n",
    "        pretrained=False,\n",
    "        num_classes=2,\n",
    "        drop_rate=DROP_RATE\n",
    "    ).to(DEVICE)\n",
    "\n",
    "    if best_global_state is not None:\n",
    "        best_model.load_state_dict(best_global_state)\n",
    "        logger.info(f\"Loaded best fold state (fold {best_global_fold}) for {model_name}.\")\n",
    "\n",
    "    training_time = time.time() - start_time\n",
    "\n",
    "    return {\n",
    "        'model': best_model,\n",
    "        'all_fold_metrics': current_fold_metrics,\n",
    "        'training_time': training_time,\n",
    "        'best_fold': best_global_fold\n",
    "    }\n",
    "\n",
    "# Full training for top models\n",
    "full_training_results = {}\n",
    "for model_name in top_models:\n",
    "    result = train_model_with_kfold(model_name, models_info[model_name])\n",
    "    full_training_results[model_name] = result\n",
    "\n",
    "# Final evaluation on test set\n",
    "logger.info(\"\\nEvaluating fully trained models on test set...\")\n",
    "\n",
    "final_test_results = {}\n",
    "for model_name, training_result in full_training_results.items():\n",
    "    logger.info(f\"Evaluating {model_name} on test set...\")\n",
    "    model = training_result['model']\n",
    "    criterion_test = nn.CrossEntropyLoss()\n",
    "\n",
    "    # Evaluate the model\n",
    "    metrics, targets_test, predictions_test, probabilities_test = evaluate_model_inline(\n",
    "        model, test_loader, criterion_test\n",
    "    )\n",
    "\n",
    "    # Log detailed metrics\n",
    "    logger.info(f\"Test Results for {model_name}:\")\n",
    "    logger.info(f\"  Accuracy: {metrics['accuracy']:.4f}\")\n",
    "    logger.info(f\"  F1 Score: {metrics['f1_score']:.4f}\")\n",
    "    logger.info(f\"  Precision: {metrics['precision']:.4f}\")\n",
    "    logger.info(f\"  Recall: {metrics['recall']:.4f}\")\n",
    "    logger.info(f\"  ROC-AUC: {metrics.get('roc_auc', 'N/A')}\")\n",
    "    logger.info(f\"  Inference Speed: {metrics['images_per_second']:.2f} images/second\")\n",
    "\n",
    "    final_test_results[model_name] = {\n",
    "        'metrics': metrics,\n",
    "        'targets': targets_test,\n",
    "        'predictions': predictions_test,\n",
    "        'probabilities': probabilities_test\n",
    "    }\n",
    "\n",
    "    # Save test results\n",
    "    results_dir = os.path.join(str(OUTPUT_DIR), 'results')\n",
    "    os.makedirs(results_dir, exist_ok=True)\n",
    "    test_results_path = os.path.join(results_dir, f\"{model_name}_final_test_results.json\")\n",
    "\n",
    "    test_data_dict = {\n",
    "        \"targets\": [int(t) for t in targets_test],\n",
    "        \"predictions\": [int(p) for p in predictions_test],\n",
    "        \"probabilities\": [float(prob) for prob in probabilities_test],\n",
    "        \"metrics\": {k: float(v) if isinstance(v, (int, float)) else v\n",
    "                    for k, v in metrics.items() if k != 'confusion_matrix'}\n",
    "    }\n",
    "\n",
    "    with open(test_results_path, 'w') as f_out:\n",
    "        json.dump(test_data_dict, f_out, indent=4)\n",
    "\n",
    "    # Confusion matrix plot\n",
    "    if 'confusion_matrix' in metrics:\n",
    "        cm_array = np.array(metrics['confusion_matrix'])\n",
    "        plot_confusion_matrix_inline(\n",
    "            cm_array,\n",
    "            os.path.join(str(OUTPUT_DIR), 'visualizations'),\n",
    "            f\"{model_name}_final_test\"\n",
    "        )\n",
    "\n",
    "    # Save the final model\n",
    "    model_save_path = os.path.join(str(OUTPUT_DIR), 'models', f\"{model_name}_final_model.pth\")\n",
    "    torch.save(model.state_dict(), model_save_path)\n",
    "    logger.info(f\"Saved final model to {model_save_path}\")\n",
    "\n",
    "# Select best final model\n",
    "logger.info(\"\\nSelecting best final model...\")\n",
    "\n",
    "best_final_model = None\n",
    "best_final_score = -float('inf')\n",
    "\n",
    "for model_name, result in final_test_results.items():\n",
    "    metrics = result['metrics']\n",
    "\n",
    "    # Calculate balanced score for final selection\n",
    "    # Balance accuracy, F1 score, and efficiency\n",
    "    accuracy = metrics['accuracy']\n",
    "    f1_score = metrics['f1_score']\n",
    "    inference_speed = metrics['images_per_second']\n",
    "    model_size = models_info[model_name]['size_mb']\n",
    "\n",
    "    # Score formula: 40% F1, 30% accuracy, 20% speed, 10% size\n",
    "    final_score = (0.4 * f1_score +\n",
    "                   0.3 * accuracy +\n",
    "                   0.2 * min(1.0, inference_speed / 100) +\n",
    "                   0.1 * (1.0 - min(1.0, model_size / 100)))\n",
    "\n",
    "    logger.info(f\"{model_name} final score: {final_score:.4f}\")\n",
    "\n",
    "    if final_score > best_final_score:\n",
    "        best_final_score = final_score\n",
    "        best_final_model = model_name\n",
    "\n",
    "# Save comprehensive results\n",
    "final_comparison = {\n",
    "    'models': {},\n",
    "    'best_model': best_final_model,\n",
    "    'training_time': {model: result['training_time'] for model, result in full_training_results.items()}\n",
    "}\n",
    "\n",
    "for model_name in top_models:\n",
    "    test_metrics = final_test_results[model_name]['metrics']\n",
    "    model_size = models_info[model_name]['size_mb']\n",
    "    param_count = models_info[model_name]['param_count']\n",
    "\n",
    "    final_comparison['models'][model_name] = {\n",
    "        'accuracy': test_metrics['accuracy'],\n",
    "        'f1_score': test_metrics['f1_score'],\n",
    "        'precision': test_metrics['precision'],\n",
    "        'recall': test_metrics['recall'],\n",
    "        'roc_auc': test_metrics.get('roc_auc', 0),\n",
    "        'size_mb': model_size,\n",
    "        'param_count': param_count,\n",
    "        'inference_time_ms': test_metrics['inference_time_per_image'] * 1000,\n",
    "        'images_per_second': test_metrics['images_per_second']\n",
    "    }\n",
    "\n",
    "# Save final comparison\n",
    "comparison_path = os.path.join(str(OUTPUT_DIR), 'results', 'final_model_comparison.json')\n",
    "with open(comparison_path, 'w') as f:\n",
    "    json.dump(final_comparison, f, indent=4)\n",
    "\n",
    "# Final summary\n",
    "logger.info(\"\\n=== FINAL RESULTS ===\")\n",
    "logger.info(f\"Best model: {best_final_model}\")\n",
    "logger.info(f\"  - Size: {models_info[best_final_model]['size_mb']:.2f} MB\")\n",
    "logger.info(f\"  - Parameters: {models_info[best_final_model]['param_count']:,}\")\n",
    "logger.info(f\"  - Accuracy: {final_test_results[best_final_model]['metrics']['accuracy']:.4f}\")\n",
    "logger.info(f\"  - F1 Score: {final_test_results[best_final_model]['metrics']['f1_score']:.4f}\")\n",
    "logger.info(f\"  - Inference: {final_test_results[best_final_model]['metrics']['images_per_second']:.2f} img/sec\")\n",
    "\n",
    "# Create summary report file\n",
    "with open(os.path.join(str(OUTPUT_DIR), 'best_model_summary.txt'), 'w') as f:\n",
    "    f.write(\"=== COFFEE BEAN DEFECT DETECTION - BEST MODEL SUMMARY ===\\n\\n\")\n",
    "    f.write(f\"Best Model: {best_final_model}\\n\")\n",
    "    f.write(f\"Model Size: {models_info[best_final_model]['size_mb']:.2f} MB\\n\")\n",
    "    f.write(f\"Parameters: {models_info[best_final_model]['param_count']:,}\\n\")\n",
    "    f.write(f\"Accuracy: {final_test_results[best_final_model]['metrics']['accuracy']:.4f}\\n\")\n",
    "    f.write(f\"F1 Score: {final_test_results[best_final_model]['metrics']['f1_score']:.4f}\\n\")\n",
    "    f.write(f\"Precision: {final_test_results[best_final_model]['metrics']['precision']:.4f}\\n\")\n",
    "    f.write(f\"Recall: {final_test_results[best_final_model]['metrics']['recall']:.4f}\\n\")\n",
    "    f.write(f\"Inference Time: {final_test_results[best_final_model]['metrics']['inference_time_per_image']*1000:.2f} ms/image\\n\")\n",
    "    f.write(f\"Processing Speed: {final_test_results[best_final_model]['metrics']['images_per_second']:.2f} images/second\\n\\n\")\n",
    "    f.write(\"=== Model Comparison ===\\n\")\n",
    "\n",
    "    for model_name in top_models:\n",
    "        metrics = final_test_results[model_name]['metrics']\n",
    "        f.write(f\"{model_name}:\\n\")\n",
    "        f.write(f\"  - Size: {models_info[model_name]['size_mb']:.2f} MB\\n\")\n",
    "        f.write(f\"  - F1 Score: {metrics['f1_score']:.4f}\\n\")\n",
    "        f.write(f\"  - Speed: {metrics['images_per_second']:.2f} img/sec\\n\\n\")\n",
    "\n",
    "logger.info(\"Training pipeline completed successfully!\")\n",
    "\n",
    "# Final cleanup\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.empty_cache()\n",
    "gc.collect()\n",
    "logger.info(\"Cleanup done.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
