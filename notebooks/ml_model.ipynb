{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-08T07:50:48.556670Z",
     "start_time": "2025-04-08T07:01:58.436202Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Import necessary libraries\n",
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time\n",
    "import json\n",
    "from datetime import datetime\n",
    "import psutil\n",
    "\n",
    "# Skimage & Mahotas for feature extraction\n",
    "from skimage.feature import graycomatrix, graycoprops\n",
    "from skimage.measure import regionprops\n",
    "import mahotas as mt\n",
    "\n",
    "# Scikit-learn\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import StratifiedKFold, cross_validate, RandomizedSearchCV\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, precision_score, recall_score, f1_score,\n",
    "    roc_auc_score, confusion_matrix, make_scorer\n",
    ")\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import (\n",
    "    RandomForestClassifier, ExtraTreesClassifier\n",
    ")\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "# Random distributions for hyperparameter optimization\n",
    "from scipy.stats import randint, uniform\n",
    "\n",
    "# Add XGBoost\n",
    "import xgboost as xgb\n",
    "\n",
    "import joblib\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import ConfusionMatrixDisplay, RocCurveDisplay\n",
    "\n",
    "# Global Configuration\n",
    "ROOT_OUTPUT_DIR = \"D:\\\\iate_project\\\\data\\\\results\\\\ml_output\"\n",
    "FEATURE_EXTRACTION_DIR = os.path.join(ROOT_OUTPUT_DIR, 'ml_features', 'ml_feature_extraction')\n",
    "RESULTS_DIR = os.path.join(ROOT_OUTPUT_DIR, 'ml_classification_results')\n",
    "MODELS_DIR = os.path.join(RESULTS_DIR, 'ml_models')\n",
    "PLOTS_DIR = os.path.join(RESULTS_DIR, 'ml_plots')\n",
    "\n",
    "# Create necessary directories\n",
    "for d in [ROOT_OUTPUT_DIR, FEATURE_EXTRACTION_DIR, RESULTS_DIR, MODELS_DIR, PLOTS_DIR]:\n",
    "    os.makedirs(d, exist_ok=True)\n",
    "\n",
    "# Hyperparameter tuning settings\n",
    "N_ITER_SEARCH = 15  # Number of parameter settings sampled in RandomizedSearchCV\n",
    "CV_FOLDS = 5        # Number of cross-validation folds for hyperparameter tuning\n",
    "\n",
    "# Define base classifiers for hyperparameter optimization\n",
    "BASE_CLASSIFIERS = {\n",
    "    'SVM': {\n",
    "        'model': SVC(probability=True, random_state=42, cache_size=1000),\n",
    "        'param_dist': {\n",
    "            'C': uniform(50, 200),\n",
    "            'gamma': ['scale', 'auto', 0.001, 0.01, 0.1],\n",
    "            'kernel': ['rbf', 'poly']\n",
    "        },\n",
    "        'expects_proba': True,\n",
    "    },\n",
    "    'RandomForest': {\n",
    "        'model': RandomForestClassifier(n_jobs=-1, random_state=42),\n",
    "        'param_dist': {\n",
    "            'n_estimators': randint(100, 500),\n",
    "            'max_depth': [None, 10, 20, 30, 40, 50],\n",
    "            'min_samples_split': randint(2, 20),\n",
    "            'min_samples_leaf': randint(1, 10),\n",
    "            'max_features': ['sqrt', 'log2', None]\n",
    "        },\n",
    "        'expects_proba': True,\n",
    "    },\n",
    "    'XGBoost': {\n",
    "        'model': xgb.XGBClassifier(objective='binary:logistic', random_state=42, n_jobs=-1),\n",
    "        'param_dist': {\n",
    "            'n_estimators': randint(100, 500),\n",
    "            'learning_rate': [0.01, 0.05, 0.1, 0.2, 0.3],\n",
    "            'max_depth': randint(3, 10),\n",
    "            'subsample': uniform(0.6, 0.4),\n",
    "            'colsample_bytree': uniform(0.6, 0.4)\n",
    "        },\n",
    "        'expects_proba': True,\n",
    "    },\n",
    "    'ExtraTrees': {\n",
    "        'model': ExtraTreesClassifier(n_jobs=-1, random_state=42),\n",
    "        'param_dist': {\n",
    "            'n_estimators': randint(100, 600),\n",
    "            'max_depth': [None, 10, 20, 30, 40, 50],\n",
    "            'min_samples_split': randint(2, 20),\n",
    "            'min_samples_leaf': randint(1, 10),\n",
    "            'max_features': ['sqrt', 'log2', None]\n",
    "        },\n",
    "        'expects_proba': True\n",
    "    },\n",
    "    'KNN': {\n",
    "        'model': KNeighborsClassifier(n_jobs=-1),\n",
    "        'param_dist': {\n",
    "            'n_neighbors': randint(3, 15),\n",
    "            'weights': ['uniform', 'distance'],\n",
    "            'metric': ['euclidean', 'manhattan', 'minkowski']\n",
    "        },\n",
    "        'expects_proba': True,\n",
    "    },\n",
    "    'DecisionTree': {\n",
    "        'model': DecisionTreeClassifier(random_state=42),\n",
    "        'param_dist': {\n",
    "            'max_depth': [None, 10, 20, 30, 40, 50],\n",
    "            'min_samples_split': randint(2, 20),\n",
    "            'min_samples_leaf': randint(1, 10),\n",
    "            'criterion': ['gini', 'entropy']\n",
    "        },\n",
    "        'expects_proba': True\n",
    "    }\n",
    "}\n",
    "\n",
    "# Define feature categories with more accurate naming\n",
    "FEATURE_CATEGORIES = {\n",
    "    'color_channel_texture': ['color_texture_'],  # Haralick texture features computed on RGB channels\n",
    "    'texture': ['glcm_'],                         # Texture features using GLCM on grayscale image\n",
    "    'shape': ['area', 'perimeter', 'eccentricity', 'extent', 'solidity']  # Shape features\n",
    "}\n",
    "\n",
    "# Record resource usage at the beginning\n",
    "process = psutil.Process(os.getpid())\n",
    "cpu_usage_before = psutil.cpu_percent(interval=None)\n",
    "mem_info_before = process.memory_info()\n",
    "memory_usage_mb_before = mem_info_before.rss / 1024 / 1024\n",
    "\n",
    "start_time_script = time.time()\n",
    "\n",
    "# Feature Extraction Configuration\n",
    "do_feature_extraction = True\n",
    "\n",
    "# Define path structure based on dataset description\n",
    "raw_dataset_path = \"D:\\\\iate_project\\\\data\\\\raw\"\n",
    "output_dir = FEATURE_EXTRACTION_DIR\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "df = None\n",
    "\n",
    "if do_feature_extraction:\n",
    "    print(\"\\n\" + \"-\"*70)\n",
    "    print(\"     STAGE 1: FEATURE EXTRACTION\")\n",
    "    print(\"-\"*70)\n",
    "\n",
    "    # Timers for each method\n",
    "    method_times = {\n",
    "        'color_channel_texture': 0.0,\n",
    "        'glcm': 0.0,\n",
    "        'region_props': 0.0\n",
    "    }\n",
    "\n",
    "    # Lists to hold extracted data\n",
    "    feats_list = []\n",
    "    labels_ = []\n",
    "    paths_ = []\n",
    "    splits_ = []\n",
    "\n",
    "    t_start_fe = time.time()\n",
    "\n",
    "    # Create storage for class distribution info\n",
    "    class_counts = {'train': {'normal': 0, 'defect': 0},\n",
    "                    'test': {'normal': 0, 'defect': 0}}\n",
    "\n",
    "    # Process image directories based on the specified structure\n",
    "    classes = ['normal', 'defect']\n",
    "    processing_types = ['dry', 'honey', 'wet']\n",
    "    roast_levels = ['dark', 'light', 'medium']\n",
    "\n",
    "    # Define train/test split ratio (70% train, 30% test)\n",
    "    train_ratio = 0.7\n",
    "\n",
    "    # Count total files for progress tracking\n",
    "    total_files_count = 0\n",
    "    for class_name in classes:\n",
    "        for proc_type in processing_types:\n",
    "            for roast in roast_levels:\n",
    "                subfolder_path = os.path.join(raw_dataset_path, class_name, proc_type, roast)\n",
    "                if os.path.exists(subfolder_path):\n",
    "                    image_files = [f for f in os.listdir(subfolder_path)\n",
    "                                  if f.lower().endswith(('.png', '.jpg', '.jpeg'))]\n",
    "                    total_files_count += len(image_files)\n",
    "\n",
    "    print(f\"Total images to process: {total_files_count}\")\n",
    "    print(\"Starting feature extraction (this may take some time)...\")\n",
    "\n",
    "    # Track processing times and counts for occasional updates\n",
    "    last_update_time = time.time()\n",
    "    processed_since_update = 0\n",
    "    total_processed = 0\n",
    "\n",
    "    # Walk through the directory structure and extract features\n",
    "    for class_name in classes:\n",
    "        for proc_type in processing_types:\n",
    "            for roast in roast_levels:\n",
    "                # Construct the path to the current subfolder\n",
    "                subfolder_path = os.path.join(raw_dataset_path, class_name, proc_type, roast)\n",
    "\n",
    "                if not os.path.exists(subfolder_path):\n",
    "                    continue\n",
    "\n",
    "                # Get list of image files in the current subfolder\n",
    "                image_files = [f for f in os.listdir(subfolder_path)\n",
    "                              if f.lower().endswith(('.png', '.jpg', '.jpeg'))]\n",
    "\n",
    "                if not image_files:\n",
    "                    continue\n",
    "\n",
    "                # Shuffle files for random split\n",
    "                np.random.seed(42)  # For reproducibility\n",
    "                np.random.shuffle(image_files)\n",
    "\n",
    "                # Split into train and test\n",
    "                split_idx = int(len(image_files) * train_ratio)\n",
    "                train_files = image_files[:split_idx]\n",
    "                test_files = image_files[split_idx:]\n",
    "\n",
    "                # Update class distribution counts\n",
    "                class_counts['train'][class_name] += len(train_files)\n",
    "                class_counts['test'][class_name] += len(test_files)\n",
    "\n",
    "                # Process each split\n",
    "                for split_name, files in [('train', train_files), ('test', test_files)]:\n",
    "                    batch_start = time.time()\n",
    "                    batch_size = len(files)\n",
    "\n",
    "                    # Show batch progress in console\n",
    "                    print(f\"Processing {batch_size} images from {class_name}/{proc_type}/{roast}/{split_name}\")\n",
    "\n",
    "                    for file_ in files:\n",
    "                        # Construct full path to the image\n",
    "                        img_path = os.path.join(subfolder_path, file_)\n",
    "\n",
    "                        # Read and process the image\n",
    "                        try:\n",
    "                            # Read color image\n",
    "                            color_img_ = cv2.imread(img_path, cv2.IMREAD_COLOR)\n",
    "                            if color_img_ is None:\n",
    "                                total_processed += 1\n",
    "                                processed_since_update += 1\n",
    "                                continue\n",
    "\n",
    "                            # Convert BGR to RGB\n",
    "                            color_img_ = cv2.cvtColor(color_img_, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "                            # Create grayscale image\n",
    "                            gray_img_ = cv2.cvtColor(color_img_, cv2.COLOR_RGB2GRAY)\n",
    "\n",
    "                            # Create threshold image (Otsu's method)\n",
    "                            _, thresh_img_ = cv2.threshold(gray_img_, 0, 255,\n",
    "                                                        cv2.THRESH_BINARY + cv2.THRESH_OTSU)\n",
    "\n",
    "                            # Extract features\n",
    "                            # 1. Color channel texture features (Haralick on RGB channels)\n",
    "                            t0_ = time.time()\n",
    "                            color_texture_features = []\n",
    "                            for d in [1, 2, 3]:  # Distances\n",
    "                                for i in range(3):  # RGB channels\n",
    "                                    channel = color_img_[:, :, i]\n",
    "                                    haralick = mt.features.haralick(channel, distance=d)\n",
    "                                    color_texture_features.extend(haralick.mean(axis=0).tolist())\n",
    "                            cost_c = time.time() - t0_\n",
    "                            method_times['color_channel_texture'] += cost_c\n",
    "\n",
    "                            # 2. GLCM features for grayscale texture analysis\n",
    "                            t1_ = time.time()\n",
    "                            glcm_ = graycomatrix(\n",
    "                                gray_img_,\n",
    "                                distances=[1, 2, 3],\n",
    "                                angles=[0, np.pi/4, np.pi/2, 3*np.pi/4],\n",
    "                                symmetric=True,\n",
    "                                normed=True\n",
    "                            )\n",
    "                            glcm_features = []\n",
    "                            props = ['contrast', 'dissimilarity', 'homogeneity', 'energy', 'correlation']\n",
    "                            for prop in props:\n",
    "                                val_ = graycoprops(glcm_, prop)\n",
    "                                glcm_features.extend(val_.flatten().tolist())\n",
    "                            cost_g = time.time() - t1_\n",
    "                            method_times['glcm'] += cost_g\n",
    "\n",
    "                            # 3. Region properties - shape features\n",
    "                            t2_ = time.time()\n",
    "                            regions = regionprops(thresh_img_.astype(np.uint8))\n",
    "                            if not regions:\n",
    "                                region_features = [0] * 5\n",
    "                            else:\n",
    "                                props_ = regions[0]\n",
    "                                region_features = [\n",
    "                                    props_.area,\n",
    "                                    props_.perimeter,\n",
    "                                    props_.eccentricity,\n",
    "                                    props_.extent,\n",
    "                                    props_.solidity\n",
    "                                ]\n",
    "                            cost_r = time.time() - t2_\n",
    "                            method_times['region_props'] += cost_r\n",
    "\n",
    "                            # Combine all features\n",
    "                            flatten_vec = color_texture_features + glcm_features + region_features\n",
    "                            feats_list.append(flatten_vec)\n",
    "                            labels_.append(class_name)\n",
    "                            paths_.append(os.path.join(class_name, proc_type, roast, file_))\n",
    "                            splits_.append(split_name)\n",
    "\n",
    "                        except Exception as e:\n",
    "                            print(f\"Error processing {img_path}: {str(e)}\")\n",
    "\n",
    "                        # Update progress tracking\n",
    "                        total_processed += 1\n",
    "                        processed_since_update += 1\n",
    "\n",
    "                        # Provide occasional progress updates\n",
    "                        current_time = time.time()\n",
    "                        if current_time - last_update_time > 60:  # Update every minute\n",
    "                            percent_done = (total_processed / total_files_count) * 100\n",
    "                            elapsed = current_time - t_start_fe\n",
    "                            rate = processed_since_update / (current_time - last_update_time)\n",
    "                            remaining = (total_files_count - total_processed) / rate if rate > 0 else 0\n",
    "\n",
    "                            print(f\"Progress: {total_processed}/{total_files_count} images ({percent_done:.1f}%), \"\n",
    "                                  f\"Rate: {rate:.1f} img/sec, Est. remaining: {remaining/60:.1f} min\")\n",
    "\n",
    "                            last_update_time = current_time\n",
    "                            processed_since_update = 0\n",
    "\n",
    "                    # Batch completion summary\n",
    "                    batch_time = time.time() - batch_start\n",
    "                    print(f\"Completed batch in {batch_time:.1f} seconds ({batch_size/batch_time:.1f} img/sec)\")\n",
    "\n",
    "    print(\"Feature extraction completed\")\n",
    "\n",
    "    # Check if we got any data\n",
    "    if not feats_list:\n",
    "        print(\"ERROR: No features were extracted.\")\n",
    "        import sys\n",
    "        sys.exit(-1)\n",
    "\n",
    "    # Generate feature column names with updated naming for color features\n",
    "    feat_names = (\n",
    "        [f'color_texture_{i}' for i in range(13*3*3)] +  # 117 color channel texture features\n",
    "        [f'glcm_{j}' for j in range(5*3*4)] +           # 60 GLCM texture features\n",
    "        ['area', 'perimeter', 'eccentricity', 'extent', 'solidity']  # 5 shape features\n",
    "    )\n",
    "\n",
    "    # Create DataFrame\n",
    "    df = pd.DataFrame(feats_list, columns=feat_names)\n",
    "    df['label'] = labels_\n",
    "    df['image_path'] = paths_\n",
    "    df['split'] = splits_\n",
    "\n",
    "    # Split and scale\n",
    "    print(\"Scaling features...\")\n",
    "    train_df = df[df['split'] == 'train']\n",
    "    test_df = df[df['split'] == 'test']\n",
    "    scaler = StandardScaler()\n",
    "    train_mat = scaler.fit_transform(train_df[feat_names])\n",
    "    test_mat = scaler.transform(test_df[feat_names])\n",
    "    df.loc[df['split'] == 'train', feat_names] = train_mat\n",
    "    df.loc[df['split'] == 'test', feat_names] = test_mat\n",
    "\n",
    "    # Save scaler info\n",
    "    sc_info = {\n",
    "        'mean': scaler.mean_.tolist(),\n",
    "        'scale': scaler.scale_.tolist(),\n",
    "        'feature_names': feat_names\n",
    "    }\n",
    "    with open(os.path.join(output_dir, 'scaler_params.json'), 'w') as f_:\n",
    "        json.dump(sc_info, f_, indent=4)\n",
    "\n",
    "    # Summaries\n",
    "    n_images_ = len(df)\n",
    "    total_spent_ = time.time() - t_start_fe\n",
    "    avg_times_ = {k: (method_times[k]/n_images_) for k in method_times}\n",
    "    method_times['total_time'] = total_spent_\n",
    "    method_times['average_times'] = avg_times_\n",
    "\n",
    "    # Save CSV of extracted features\n",
    "    output_csv_path = os.path.join(output_dir, 'coffee_bean_features.csv')\n",
    "    df.to_csv(output_csv_path, index=False)\n",
    "\n",
    "    # Build metadata\n",
    "    class_dist = df.groupby(['split', 'label']).size()\n",
    "    class_dist_dict = {\n",
    "        f\"{sp}_{lb}\": cnt\n",
    "        for (sp, lb), cnt in class_dist.items()\n",
    "    }\n",
    "    feature_dims = {\n",
    "        'color_channel_texture': 13*3*3,  # 117 Haralick features on RGB channels\n",
    "        'glcm': 5*3*4,                    # 60 GLCM features\n",
    "        'region_props': 5                 # 5 shape features\n",
    "    }\n",
    "    meta_ = {\n",
    "        'extraction_date': datetime.now().strftime('%Y-%m-%d %H:%M:%S'),\n",
    "        'processing_time': {\n",
    "            'total_time': f\"{method_times['total_time']:.2f} seconds\",\n",
    "            'method_breakdown': {\n",
    "                k: f\"{method_times[k]:.2f} seconds\"\n",
    "                for k in method_times\n",
    "                if k not in ['total_time', 'average_times']\n",
    "            },\n",
    "            'average_times': {\n",
    "                k: f\"{avg_times_[k]:.4f} seconds/image\"\n",
    "                for k in avg_times_\n",
    "                if k not in ['total_time', 'average_times']\n",
    "            }\n",
    "        },\n",
    "        'dataset_statistics': {\n",
    "            'total_samples': len(df),\n",
    "            'class_distribution': class_dist_dict,\n",
    "            'feature_dimensions': feature_dims,\n",
    "            'total_dimensions': sum(feature_dims.values())\n",
    "        },\n",
    "        'parameters': {\n",
    "            'color_channel_texture_distances': [1, 2, 3],\n",
    "            'glcm_distances': [1, 2, 3],\n",
    "            'glcm_angles': [0, 45, 90, 135]\n",
    "        }\n",
    "    }\n",
    "\n",
    "    meta_path_ = os.path.join(output_dir, 'extraction_metadata.json')\n",
    "    with open(meta_path_, 'w') as f_:\n",
    "        json.dump(meta_, f_, indent=4)\n",
    "\n",
    "    # Print extraction summary\n",
    "    print(\"\\n\" + \"-\"*70)\n",
    "    print(\"                  FEATURE EXTRACTION SUMMARY\")\n",
    "    print(\"-\"*70)\n",
    "    print(f\"Total samples: {len(df)} images\")\n",
    "    print(f\"Feature dimensionality: {meta_['dataset_statistics']['total_dimensions']} features\")\n",
    "    print(f\"  - Color channel texture features: {feature_dims['color_channel_texture']} dimensions\")\n",
    "    print(f\"  - Texture features (GLCM): {feature_dims['glcm']} dimensions\")\n",
    "    print(f\"  - Shape features: {feature_dims['region_props']} dimensions\")\n",
    "    print(\"\\nClass distribution:\")\n",
    "\n",
    "    for s_ in ['train', 'test']:\n",
    "        dist_s_ = df[df['split'] == s_]['label'].value_counts()\n",
    "        print(f\"  {s_:<5} set => normal: {dist_s_.get('normal', 0):<5}, defect: {dist_s_.get('defect', 0):<5}\")\n",
    "\n",
    "    print(f\"\\nTotal processing time: {method_times['total_time']:.1f} seconds ({n_images_/method_times['total_time']:.1f} images/sec)\")\n",
    "\n",
    "else:\n",
    "    print(\"Skipping feature extraction. Loading features from CSV...\")\n",
    "    csv_path = os.path.join(FEATURE_EXTRACTION_DIR, \"coffee_bean_features.csv\")\n",
    "    if os.path.exists(csv_path):\n",
    "        df = pd.read_csv(csv_path)\n",
    "        print(f\"Loaded {len(df)} samples from {csv_path}\")\n",
    "    else:\n",
    "        print(f\"ERROR: No feature CSV found at {csv_path}. Cannot proceed.\")\n",
    "        import sys\n",
    "        sys.exit(-1)\n",
    "\n",
    "if df is None:\n",
    "    print(\"ERROR: No data available for classification. Aborting.\")\n",
    "    import sys\n",
    "    sys.exit(-1)\n",
    "\n",
    "feat_cols = [c for c in df.columns if c not in ['label', 'image_path', 'split']]\n",
    "train_df_ = df[df['split'] == 'train']\n",
    "test_df_ = df[df['split'] == 'test']\n",
    "\n",
    "X_train = train_df_[feat_cols].values\n",
    "X_test = test_df_[feat_cols].values\n",
    "label_map = {'normal': 0, 'defect': 1}\n",
    "y_train = np.array([label_map[l] for l in train_df_['label']])\n",
    "y_test = np.array([label_map[l] for l in test_df_['label']])\n",
    "\n",
    "print(f\"Dataset prepared for classification:\")\n",
    "print(f\"  - Train set: {len(X_train)} samples\")\n",
    "print(f\"  - Test set: {len(X_test)} samples\")\n",
    "print(f\"  - Features: {len(feat_cols)} dimensions\")\n",
    "\n",
    "# Define feature combinations to evaluate (updated with new feature category name)\n",
    "feature_combos = [\n",
    "    ['color_channel_texture'],\n",
    "    ['texture'],\n",
    "    ['shape'],\n",
    "    ['color_channel_texture', 'texture'],\n",
    "    ['color_channel_texture', 'shape'],\n",
    "    ['texture', 'shape'],\n",
    "    ['color_channel_texture', 'texture', 'shape']\n",
    "]\n",
    "\n",
    "# Setup for results collection\n",
    "all_results = {}\n",
    "best_model_script = {\n",
    "    'score': -1,\n",
    "    'combo': None,\n",
    "    'clf_name': None,\n",
    "    'model': None,\n",
    "    'results': None,\n",
    "    'features': None,\n",
    "    'best_params': None\n",
    "}\n",
    "\n",
    "print(\"\\nEvaluating feature combinations with hyperparameter tuning...\")\n",
    "print(f\"This will test {len(feature_combos)} feature combinations × {len(BASE_CLASSIFIERS)} classifiers\")\n",
    "print(f\"Each classifier will be optimized using RandomizedSearchCV with {N_ITER_SEARCH} iterations\")\n",
    "\n",
    "# Set up evaluation progress tracking\n",
    "total_evals = len(feature_combos) * len(BASE_CLASSIFIERS)\n",
    "start_eval_time = time.time()\n",
    "completed_evals = 0\n",
    "\n",
    "# Loop through each feature combination\n",
    "for combo_ in feature_combos:\n",
    "    # Select features for this combination\n",
    "    idx_sel_ = []\n",
    "    dims_ = {}\n",
    "\n",
    "    # Select feature indices for current combination\n",
    "    for cat_ in combo_:\n",
    "        cat_cols_ = []\n",
    "        for prefix_ in FEATURE_CATEGORIES[cat_]:\n",
    "            if prefix_.endswith('_'):\n",
    "                found_ = [i for i, c in enumerate(feat_cols) if c.startswith(prefix_)]\n",
    "            else:\n",
    "                found_ = [i for i, c in enumerate(feat_cols) if c == prefix_]\n",
    "            cat_cols_.extend(found_)\n",
    "        dims_[cat_] = len(cat_cols_)\n",
    "        idx_sel_.extend(cat_cols_)\n",
    "\n",
    "    # Extract selected features\n",
    "    X_train_sel_ = X_train[:, idx_sel_]\n",
    "    X_test_sel_ = X_test[:, idx_sel_]\n",
    "    sel_names_ = [feat_cols[i] for i in idx_sel_]\n",
    "\n",
    "    combo_name_ = '+'.join(combo_)\n",
    "    all_results[combo_name_] = {}\n",
    "\n",
    "    # Compact feature info log\n",
    "    feature_info = \", \".join([f\"{c}: {dims_[c]}\" for c in combo_])\n",
    "    print(f\"\\nEvaluating feature combination: '{combo_name_}' ({feature_info})\")\n",
    "\n",
    "    # No feature selection - using all features\n",
    "    print(\"Using all features (no feature selection)\")\n",
    "\n",
    "    # All features are kept - create a mask of all ones\n",
    "    selected_mask = np.ones(len(sel_names_), dtype=bool)\n",
    "\n",
    "    # Evaluate each classifier with this feature combination\n",
    "    for clf_name_, clf_info_ in BASE_CLASSIFIERS.items():\n",
    "        eval_start = time.time()\n",
    "\n",
    "        print(f\"Optimizing {clf_name_} with hyperparameter tuning...\")\n",
    "\n",
    "        # Create a pipeline with just the classifier\n",
    "        pipeline_ = Pipeline([\n",
    "            ('classifier', clf_info_['model'])\n",
    "        ])\n",
    "\n",
    "        # Setup parameter distributions for the pipeline\n",
    "        param_dist = {}\n",
    "        for param, value in clf_info_['param_dist'].items():\n",
    "            param_dist[f'classifier__{param}'] = value\n",
    "\n",
    "        # Create RandomizedSearchCV for hyperparameter tuning\n",
    "        random_search = RandomizedSearchCV(\n",
    "            estimator=pipeline_,\n",
    "            param_distributions=param_dist,\n",
    "            n_iter=N_ITER_SEARCH,\n",
    "            cv=CV_FOLDS,\n",
    "            scoring='f1',\n",
    "            n_jobs=-1,\n",
    "            random_state=42,\n",
    "            verbose=0\n",
    "        )\n",
    "\n",
    "        # Fit RandomizedSearchCV to find best parameters\n",
    "        random_search.fit(X_train_sel_, y_train)\n",
    "\n",
    "        # Get best model and parameters\n",
    "        best_model = random_search.best_estimator_\n",
    "        best_params = random_search.best_params_\n",
    "\n",
    "        # Print best parameters in a readable format\n",
    "        print(f\"Best parameters for {clf_name_}:\")\n",
    "        for param, value in best_params.items():\n",
    "            print(f\"  {param.replace('classifier__', '')}: {value}\")\n",
    "\n",
    "        # Cross-validation with best model\n",
    "        cv_metrics_ = {}\n",
    "        splitter_ = StratifiedKFold(n_splits=10, shuffle=True, random_state=42)\n",
    "        scoring_ = {\n",
    "            'accuracy': 'accuracy',\n",
    "            'precision': make_scorer(precision_score, zero_division=0),\n",
    "            'recall': 'recall',\n",
    "            'f1': 'f1',\n",
    "            'roc_auc': 'roc_auc'\n",
    "        }\n",
    "        cv_results_ = cross_validate(\n",
    "            best_model, X_train_sel_, y_train,\n",
    "            scoring=scoring_, cv=splitter_, n_jobs=-1, return_train_score=True\n",
    "        )\n",
    "\n",
    "        # Process CV results\n",
    "        for metric, scores in [(k.replace('test_', ''), cv_results_[k])\n",
    "                               for k in cv_results_ if k.startswith('test_')]:\n",
    "            cv_metrics_[metric] = {\n",
    "                'mean': float(scores.mean()),\n",
    "                'std': float(scores.std()),\n",
    "                'values': scores.tolist()\n",
    "            }\n",
    "\n",
    "        # Predict test set\n",
    "        y_pred_ = best_model.predict(X_test_sel_)\n",
    "        y_proba_ = None\n",
    "        if clf_info_['expects_proba']:\n",
    "            y_proba_ = best_model.predict_proba(X_test_sel_)[:, 1]\n",
    "\n",
    "        # Compute test metrics\n",
    "        metrics_ = {\n",
    "            'accuracy': accuracy_score(y_test, y_pred_),\n",
    "            'precision': precision_score(y_test, y_pred_),\n",
    "            'recall': recall_score(y_test, y_pred_),\n",
    "            'f1': f1_score(y_test, y_pred_),\n",
    "            'roc_auc': roc_auc_score(y_test, y_proba_) if y_proba_ is not None else None\n",
    "        }\n",
    "\n",
    "        # Get feature importances if available\n",
    "        final_clf_ = best_model.named_steps['classifier']\n",
    "        feat_imp_ = None\n",
    "        if hasattr(final_clf_, 'feature_importances_'):\n",
    "            feat_imp_ = final_clf_.feature_importances_\n",
    "        elif hasattr(final_clf_, 'coef_'):\n",
    "            feat_imp_ = np.abs(final_clf_.coef_[0])\n",
    "\n",
    "        # Record total time\n",
    "        total_t_ = time.time() - eval_start\n",
    "\n",
    "        # Store results\n",
    "        result_dict_ = {\n",
    "            'model_name': clf_name_,\n",
    "            'cross_validation': cv_metrics_,\n",
    "            'test_performance': metrics_,\n",
    "            'timing': {\n",
    "                'total_time': total_t_\n",
    "            },\n",
    "            'feature_importance': feat_imp_.tolist() if feat_imp_ is not None else None,\n",
    "            'best_params': best_params,\n",
    "            'selected_features': selected_mask.tolist() if hasattr(selected_mask, 'tolist') else None\n",
    "        }\n",
    "        all_results[combo_name_][clf_name_] = result_dict_\n",
    "\n",
    "        # Track the best model by F1 score\n",
    "        cur_f1_ = metrics_['f1']\n",
    "        if cur_f1_ > best_model_script['score']:\n",
    "            best_model_script['score'] = cur_f1_\n",
    "            best_model_script['combo'] = combo_\n",
    "            best_model_script['clf_name'] = clf_name_\n",
    "            best_model_script['model'] = best_model\n",
    "            best_model_script['results'] = result_dict_\n",
    "            best_model_script['features'] = sel_names_\n",
    "            best_model_script['best_params'] = best_params\n",
    "\n",
    "        # Update progress\n",
    "        completed_evals += 1\n",
    "        elapsed = time.time() - start_eval_time\n",
    "        rate = completed_evals / elapsed if elapsed > 0 else 0\n",
    "        remaining = (total_evals - completed_evals) / rate if rate > 0 else 0\n",
    "\n",
    "        # Print result with clear formatting, highlight best so far\n",
    "        is_best = (best_model_script['clf_name'] == clf_name_ and\n",
    "                   best_model_script['combo'] == combo_)\n",
    "\n",
    "        best_marker = \"★ \" if is_best else \"  \"\n",
    "\n",
    "        print(f\"{best_marker}{clf_name_:15} | \"\n",
    "              f\"F1: {metrics_['f1']:.4f} | \"\n",
    "              f\"Acc: {metrics_['accuracy']:.4f} | \"\n",
    "              f\"Prec: {metrics_['precision']:.4f} | \"\n",
    "              f\"Rec: {metrics_['recall']:.4f} | \"\n",
    "              f\"Time: {total_t_:.1f}s\")\n",
    "\n",
    "print(\"All models evaluated successfully.\")\n",
    "\n",
    "# Create a summary table\n",
    "summary_table = []\n",
    "for combo_key, model_dict in all_results.items():\n",
    "    for model_k, model_res in model_dict.items():\n",
    "        summary_table.append({\n",
    "            'Feature Combo': combo_key,\n",
    "            'Model': model_k,\n",
    "            'Accuracy': f\"{model_res['test_performance']['accuracy']:.4f}\",\n",
    "            'Precision': f\"{model_res['test_performance']['precision']:.4f}\",\n",
    "            'Recall': f\"{model_res['test_performance']['recall']:.4f}\",\n",
    "            'F1 Score': f\"{model_res['test_performance']['f1']:.4f}\",\n",
    "            'ROC AUC': (\n",
    "                f\"{model_res['test_performance']['roc_auc']:.4f}\"\n",
    "                if model_res['test_performance']['roc_auc'] is not None\n",
    "                else 'N/A'\n",
    "            )\n",
    "        })\n",
    "\n",
    "summary_df = pd.DataFrame(summary_table)\n",
    "\n",
    "# Print highly visible results banner\n",
    "print(f\"\\nBEST MODEL: {best_model_script['clf_name']} with {'+'.join(best_model_script['combo'])}\")\n",
    "print(f\"F1 SCORE: {best_model_script['score']:.4f}\")\n",
    "print(f\"ACCURACY: {best_model_script['results']['test_performance']['accuracy']:.4f}\")\n",
    "print(f\"PRECISION: {best_model_script['results']['test_performance']['precision']:.4f}\")\n",
    "print(f\"RECALL: {best_model_script['results']['test_performance']['recall']:.4f}\")\n",
    "\n",
    "# Show best parameters\n",
    "print(\"\\nBEST HYPERPARAMETERS:\")\n",
    "for param, value in best_model_script['best_params'].items():\n",
    "    print(f\"  {param.replace('classifier__', '')}: {value}\")\n",
    "\n",
    "# Show top 3 models\n",
    "print(\"\\nTOP 3 MODELS BY F1 SCORE:\")\n",
    "top_models = sorted(\n",
    "    [(combo, model, results['test_performance']['f1'])\n",
    "     for combo, models in all_results.items()\n",
    "     for model, results in models.items()],\n",
    "    key=lambda x: x[2], reverse=True\n",
    ")[:3]\n",
    "\n",
    "for i, (combo, model, f1) in enumerate(top_models, 1):\n",
    "    result = all_results[combo][model]['test_performance']\n",
    "    print(f\"{i}. {model} with {combo}\")\n",
    "    print(f\"   F1: {f1:.4f} | Acc: {result['accuracy']:.4f} | \"\n",
    "          f\"Prec: {result['precision']:.4f} | Rec: {result['recall']:.4f}\")\n",
    "\n",
    "# Save results as JSON\n",
    "out_json_ = os.path.join(RESULTS_DIR, 'classification_results.json')\n",
    "with open(out_json_, 'w') as f_:\n",
    "    json.dump(all_results, f_, indent=4)\n",
    "\n",
    "# Save best model\n",
    "model_path_ = os.path.join(MODELS_DIR, 'best_model.joblib')\n",
    "joblib.dump(best_model_script['model'], model_path_)\n",
    "\n",
    "# Save feature info\n",
    "feature_info_path = os.path.join(MODELS_DIR, 'feature_info.json')\n",
    "feature_info = {\n",
    "    'feature_combination': best_model_script['combo'],\n",
    "    'features': best_model_script['features']\n",
    "}\n",
    "with open(feature_info_path, 'w') as f_:\n",
    "    json.dump(feature_info, f_, indent=4)\n",
    "\n",
    "# Save a detailed report\n",
    "best_report_ = {\n",
    "    'model_summary': {\n",
    "        'name': best_model_script['clf_name'],\n",
    "        'feature_combination': '+'.join(best_model_script['combo']),\n",
    "        'total_features': len(best_model_script['features']),\n",
    "        'best_hyperparameters': best_model_script['best_params']\n",
    "    },\n",
    "    'performance_metrics': best_model_script['results']['test_performance'],\n",
    "    'timing_information': best_model_script['results']['timing'],\n",
    "    'feature_importance': (\n",
    "        dict(zip(\n",
    "            best_model_script['features'],\n",
    "            best_model_script['results']['feature_importance']\n",
    "        )) if best_model_script['results']['feature_importance'] is not None else None\n",
    "    )\n",
    "}\n",
    "report_path_ = os.path.join(RESULTS_DIR, 'detailed_report.json')\n",
    "with open(report_path_, 'w') as f_:\n",
    "    json.dump(best_report_, f_, indent=4)\n",
    "\n",
    "print(\"\\nClassification results and best model saved to disk.\")\n",
    "\n",
    "# Generate best model plots\n",
    "# Select features for best model\n",
    "idx_sel_ = []\n",
    "for cat_ in best_model_script['combo']:\n",
    "    for prefix_ in FEATURE_CATEGORIES[cat_]:\n",
    "        if prefix_.endswith('_'):\n",
    "            found_ = [i for i, c in enumerate(feat_cols) if c.startswith(prefix_)]\n",
    "        else:\n",
    "            found_ = [i for i, c in enumerate(feat_cols) if c == prefix_]\n",
    "        idx_sel_.extend(found_)\n",
    "X_test_best_ = X_test[:, idx_sel_]\n",
    "\n",
    "# Get predictions\n",
    "y_pred_best_ = best_model_script['model'].predict(X_test_best_)\n",
    "y_proba_best_ = None\n",
    "if hasattr(best_model_script['model'].named_steps['classifier'], 'predict_proba'):\n",
    "    y_proba_best_ = best_model_script['model'].predict_proba(X_test_best_)[:,1]\n",
    "\n",
    "# Plot confusion matrix\n",
    "print(\"Generating confusion matrix...\")\n",
    "cm_path_ = os.path.join(PLOTS_DIR, 'best_model_confusion_matrix.png')\n",
    "cm_ = confusion_matrix(y_test, y_pred_best_)\n",
    "classes_ = ['normal', 'defect']\n",
    "disp_ = ConfusionMatrixDisplay(cm_, display_labels=classes_)\n",
    "fig_, ax_ = plt.subplots(figsize=(8, 7))\n",
    "disp_.plot(ax=ax_, cmap=plt.cm.Blues, colorbar=False)\n",
    "plt.title(f\"Confusion Matrix - {best_model_script['clf_name']} with {'+'.join(best_model_script['combo'])}\")\n",
    "plt.tight_layout()\n",
    "plt.savefig(cm_path_)\n",
    "plt.close()\n",
    "\n",
    "# Print confusion matrix values\n",
    "tn, fp, fn, tp = cm_.ravel()\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "print(f\"  True Negatives (normal correctly predicted): {tn}\")\n",
    "print(f\"  False Positives (normal incorrectly as defect): {fp}\")\n",
    "print(f\"  False Negatives (defect incorrectly as normal): {fn}\")\n",
    "print(f\"  True Positives (defect correctly predicted): {tp}\")\n",
    "\n",
    "# Plot ROC curve if probability estimates available\n",
    "if y_proba_best_ is not None:\n",
    "    print(\"Generating ROC curve...\")\n",
    "    roc_path_ = os.path.join(PLOTS_DIR, 'best_model_roc_curve.png')\n",
    "    disp_ = RocCurveDisplay.from_predictions(y_test, y_proba_best_)\n",
    "    fig_ = disp_.figure_\n",
    "    plt.title(f\"ROC Curve - {best_model_script['clf_name']} with {'+'.join(best_model_script['combo'])}\")\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(roc_path_)\n",
    "    plt.close()\n",
    "\n",
    "# Plot feature importances if available\n",
    "if best_model_script['results']['feature_importance'] is not None:\n",
    "    print(\"Generating feature importance visualization...\")\n",
    "    fi_path_ = os.path.join(PLOTS_DIR, 'best_model_feature_importances.png')\n",
    "    fi_abs_ = np.abs(np.array(best_model_script['results']['feature_importance']))\n",
    "    idx_sorted_ = np.argsort(fi_abs_)[::-1]\n",
    "    top_n = min(15, len(fi_abs_))\n",
    "    fi_sorted_ = fi_abs_[idx_sorted_][:top_n]\n",
    "    names_sorted_ = [best_model_script['features'][i] for i in idx_sorted_[:top_n]]\n",
    "\n",
    "    fig_, ax_ = plt.subplots(figsize=(10, 8))\n",
    "    y_pos_ = np.arange(len(fi_sorted_))\n",
    "    ax_.barh(y_pos_, fi_sorted_[::-1], align='center', color='skyblue')\n",
    "    ax_.set_yticks(y_pos_)\n",
    "    ax_.set_yticklabels(names_sorted_[::-1])\n",
    "    ax_.invert_yaxis()\n",
    "    ax_.set_xlabel('Importance')\n",
    "    ax_.set_title(f'Feature Importances - Top {top_n}')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(fi_path_)\n",
    "    plt.close()\n",
    "\n",
    "    # Print top features\n",
    "    print(\"\\nTop 5 Most Important Features:\")\n",
    "    for i in range(min(5, top_n)):\n",
    "        print(f\"  {i+1}. {names_sorted_[i]}: {fi_sorted_[i]:.4f}\")\n",
    "\n",
    "# Calculate final resource usage\n",
    "cpu_usage_after = psutil.cpu_percent(interval=None)\n",
    "mem_info_after = process.memory_info()\n",
    "memory_usage_mb_after = mem_info_after.rss / 1024 / 1024\n",
    "\n",
    "total_time_script = time.time() - start_time_script\n",
    "\n",
    "print(f\"Total execution time: {total_time_script:.1f} seconds ({total_time_script/60:.1f} minutes)\")\n",
    "print(f\"Memory usage: {memory_usage_mb_before:.1f}MB → {memory_usage_mb_after:.1f}MB\")\n",
    "print(f\"All results saved to: {RESULTS_DIR}\")"
   ],
   "id": "d93cf45c06545a6d",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "----------------------------------------------------------------------\n",
      "     STAGE 1: FEATURE EXTRACTION\n",
      "----------------------------------------------------------------------\n",
      "Total images to process: 5400\n",
      "Starting feature extraction (this may take some time)...\n",
      "Processing 280 images from normal/dry/dark/train\n",
      "Completed batch in 48.2 seconds (5.8 img/sec)\n",
      "Processing 120 images from normal/dry/dark/test\n",
      "Progress: 347/5400 images (6.4%), Rate: 5.8 img/sec, Est. remaining: 14.6 min\n",
      "Completed batch in 21.9 seconds (5.5 img/sec)\n",
      "Processing 280 images from normal/dry/light/train\n",
      "Progress: 648/5400 images (12.0%), Rate: 5.0 img/sec, Est. remaining: 15.8 min\n",
      "Completed batch in 57.0 seconds (4.9 img/sec)\n",
      "Processing 120 images from normal/dry/light/test\n",
      "Completed batch in 24.7 seconds (4.9 img/sec)\n",
      "Processing 280 images from normal/dry/medium/train\n",
      "Progress: 953/5400 images (17.6%), Rate: 5.1 img/sec, Est. remaining: 14.6 min\n",
      "Completed batch in 52.8 seconds (5.3 img/sec)\n",
      "Processing 120 images from normal/dry/medium/test\n",
      "Completed batch in 22.4 seconds (5.4 img/sec)\n",
      "Processing 280 images from normal/honey/dark/train\n",
      "Progress: 1278/5400 images (23.7%), Rate: 5.4 img/sec, Est. remaining: 12.7 min\n",
      "Completed batch in 48.8 seconds (5.7 img/sec)\n",
      "Processing 120 images from normal/honey/dark/test\n",
      "Completed batch in 21.3 seconds (5.6 img/sec)\n",
      "Processing 280 images from normal/honey/light/train\n",
      "Progress: 1620/5400 images (30.0%), Rate: 5.7 img/sec, Est. remaining: 11.1 min\n",
      "Completed batch in 47.9 seconds (5.8 img/sec)\n",
      "Processing 120 images from normal/honey/light/test\n",
      "Progress: 1974/5400 images (36.6%), Rate: 5.9 img/sec, Est. remaining: 9.7 min\n",
      "Completed batch in 19.9 seconds (6.0 img/sec)\n",
      "Processing 280 images from normal/honey/medium/train\n",
      "Completed batch in 49.0 seconds (5.7 img/sec)\n",
      "Processing 120 images from normal/honey/medium/test\n",
      "Progress: 2327/5400 images (43.1%), Rate: 5.9 img/sec, Est. remaining: 8.7 min\n",
      "Completed batch in 17.4 seconds (6.9 img/sec)\n",
      "Processing 280 images from normal/wet/dark/train\n",
      "Progress: 2676/5400 images (49.6%), Rate: 5.8 img/sec, Est. remaining: 7.8 min\n",
      "Completed batch in 50.2 seconds (5.6 img/sec)\n",
      "Processing 120 images from normal/wet/dark/test\n",
      "Completed batch in 22.1 seconds (5.4 img/sec)\n",
      "Processing 280 images from normal/wet/light/train\n",
      "Progress: 3016/5400 images (55.9%), Rate: 5.7 img/sec, Est. remaining: 7.0 min\n",
      "Completed batch in 48.1 seconds (5.8 img/sec)\n",
      "Processing 120 images from normal/wet/light/test\n",
      "Completed batch in 20.1 seconds (6.0 img/sec)\n",
      "Processing 280 images from normal/wet/medium/train\n",
      "Progress: 3368/5400 images (62.4%), Rate: 5.9 img/sec, Est. remaining: 5.8 min\n",
      "Completed batch in 48.0 seconds (5.8 img/sec)\n",
      "Processing 120 images from normal/wet/medium/test\n",
      "Completed batch in 20.3 seconds (5.9 img/sec)\n",
      "Processing 140 images from defect/dry/dark/train\n",
      "Progress: 3714/5400 images (68.8%), Rate: 5.8 img/sec, Est. remaining: 4.9 min\n",
      "Completed batch in 25.8 seconds (5.4 img/sec)\n",
      "Processing 60 images from defect/dry/dark/test\n",
      "Completed batch in 11.4 seconds (5.2 img/sec)\n",
      "Processing 140 images from defect/dry/light/train\n",
      "Completed batch in 29.1 seconds (4.8 img/sec)\n",
      "Processing 60 images from defect/dry/light/test\n",
      "Completed batch in 12.6 seconds (4.8 img/sec)\n",
      "Processing 140 images from defect/dry/medium/train\n",
      "Progress: 4010/5400 images (74.3%), Rate: 4.9 img/sec, Est. remaining: 4.7 min\n",
      "Completed batch in 27.0 seconds (5.2 img/sec)\n",
      "Processing 60 images from defect/dry/medium/test\n",
      "Completed batch in 11.3 seconds (5.3 img/sec)\n",
      "Processing 140 images from defect/honey/dark/train\n",
      "Completed batch in 23.2 seconds (6.0 img/sec)\n",
      "Processing 60 images from defect/honey/dark/test\n",
      "Progress: 4344/5400 images (80.4%), Rate: 5.6 img/sec, Est. remaining: 3.2 min\n",
      "Completed batch in 11.1 seconds (5.4 img/sec)\n",
      "Processing 140 images from defect/honey/light/train\n",
      "Completed batch in 23.6 seconds (5.9 img/sec)\n",
      "Processing 60 images from defect/honey/light/test\n",
      "Completed batch in 10.2 seconds (5.9 img/sec)\n",
      "Processing 140 images from defect/honey/medium/train\n",
      "Progress: 4691/5400 images (86.9%), Rate: 5.8 img/sec, Est. remaining: 2.0 min\n",
      "Completed batch in 24.4 seconds (5.7 img/sec)\n",
      "Processing 60 images from defect/honey/medium/test\n",
      "Completed batch in 11.2 seconds (5.4 img/sec)\n",
      "Processing 140 images from defect/wet/dark/train\n",
      "Completed batch in 24.1 seconds (5.8 img/sec)\n",
      "Processing 60 images from defect/wet/dark/test\n",
      "Completed batch in 10.1 seconds (5.9 img/sec)\n",
      "Processing 140 images from defect/wet/light/train\n",
      "Progress: 5037/5400 images (93.3%), Rate: 5.8 img/sec, Est. remaining: 1.1 min\n",
      "Completed batch in 24.2 seconds (5.8 img/sec)\n",
      "Processing 60 images from defect/wet/light/test\n",
      "Completed batch in 10.5 seconds (5.7 img/sec)\n",
      "Processing 140 images from defect/wet/medium/train\n",
      "Completed batch in 23.9 seconds (5.9 img/sec)\n",
      "Processing 60 images from defect/wet/medium/test\n",
      "Progress: 5385/5400 images (99.7%), Rate: 5.8 img/sec, Est. remaining: 0.0 min\n",
      "Completed batch in 10.2 seconds (5.9 img/sec)\n",
      "Feature extraction completed\n",
      "Scaling features...\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "                  FEATURE EXTRACTION SUMMARY\n",
      "----------------------------------------------------------------------\n",
      "Total samples: 5400 images\n",
      "Feature dimensionality: 182 features\n",
      "  - Color channel texture features: 117 dimensions\n",
      "  - Texture features (GLCM): 60 dimensions\n",
      "  - Shape features: 5 dimensions\n",
      "\n",
      "Class distribution:\n",
      "  train set => normal: 2520 , defect: 1260 \n",
      "  test  set => normal: 1080 , defect: 540  \n",
      "\n",
      "Total processing time: 964.2 seconds (5.6 images/sec)\n",
      "Dataset prepared for classification:\n",
      "  - Train set: 3780 samples\n",
      "  - Test set: 1620 samples\n",
      "  - Features: 182 dimensions\n",
      "\n",
      "Evaluating feature combinations with hyperparameter tuning...\n",
      "This will test 7 feature combinations × 6 classifiers\n",
      "Each classifier will be optimized using RandomizedSearchCV with 15 iterations\n",
      "\n",
      "Evaluating feature combination: 'color_channel_texture' (color_channel_texture: 117)\n",
      "Using all features (no feature selection)\n",
      "Optimizing SVM with hyperparameter tuning...\n",
      "Best parameters for SVM:\n",
      "  C: 152.84688768272233\n",
      "  gamma: scale\n",
      "  kernel: rbf\n",
      "★ SVM             | F1: 0.8240 | Acc: 0.8877 | Prec: 0.8623 | Rec: 0.7889 | Time: 131.6s\n",
      "Optimizing RandomForest with hyperparameter tuning...\n",
      "Best parameters for RandomForest:\n",
      "  max_depth: 30\n",
      "  max_features: sqrt\n",
      "  min_samples_leaf: 3\n",
      "  min_samples_split: 6\n",
      "  n_estimators: 406\n",
      "  RandomForest    | F1: 0.6897 | Acc: 0.8111 | Prec: 0.7623 | Rec: 0.6296 | Time: 43.4s\n",
      "Optimizing XGBoost with hyperparameter tuning...\n",
      "Best parameters for XGBoost:\n",
      "  colsample_bytree: 0.9637281608315128\n",
      "  learning_rate: 0.2\n",
      "  max_depth: 4\n",
      "  n_estimators: 459\n",
      "  subsample: 0.902144564127061\n",
      "  XGBoost         | F1: 0.7289 | Acc: 0.8296 | Prec: 0.7762 | Rec: 0.6870 | Time: 30.8s\n",
      "Optimizing ExtraTrees with hyperparameter tuning...\n",
      "Best parameters for ExtraTrees:\n",
      "  max_depth: 20\n",
      "  max_features: None\n",
      "  min_samples_leaf: 3\n",
      "  min_samples_split: 13\n",
      "  n_estimators: 154\n",
      "  ExtraTrees      | F1: 0.6816 | Acc: 0.8068 | Prec: 0.7562 | Rec: 0.6204 | Time: 10.7s\n",
      "Optimizing KNN with hyperparameter tuning...\n",
      "Best parameters for KNN:\n",
      "  metric: euclidean\n",
      "  n_neighbors: 3\n",
      "  weights: uniform\n",
      "  KNN             | F1: 0.6277 | Acc: 0.7679 | Prec: 0.6745 | Rec: 0.5870 | Time: 1.8s\n",
      "Optimizing DecisionTree with hyperparameter tuning...\n",
      "Best parameters for DecisionTree:\n",
      "  criterion: gini\n",
      "  max_depth: 30\n",
      "  min_samples_leaf: 9\n",
      "  min_samples_split: 4\n",
      "  DecisionTree    | F1: 0.5926 | Acc: 0.7420 | Prec: 0.6255 | Rec: 0.5630 | Time: 2.7s\n",
      "\n",
      "Evaluating feature combination: 'texture' (texture: 60)\n",
      "Using all features (no feature selection)\n",
      "Optimizing SVM with hyperparameter tuning...\n",
      "Best parameters for SVM:\n",
      "  C: 152.84688768272233\n",
      "  gamma: scale\n",
      "  kernel: rbf\n",
      "  SVM             | F1: 0.7393 | Acc: 0.8420 | Prec: 0.8213 | Rec: 0.6722 | Time: 568.8s\n",
      "Optimizing RandomForest with hyperparameter tuning...\n",
      "Best parameters for RandomForest:\n",
      "  max_depth: 30\n",
      "  max_features: sqrt\n",
      "  min_samples_leaf: 3\n",
      "  min_samples_split: 6\n",
      "  n_estimators: 406\n",
      "  RandomForest    | F1: 0.6156 | Acc: 0.7802 | Prec: 0.7383 | Rec: 0.5278 | Time: 30.3s\n",
      "Optimizing XGBoost with hyperparameter tuning...\n",
      "Best parameters for XGBoost:\n",
      "  colsample_bytree: 0.749816047538945\n",
      "  learning_rate: 0.3\n",
      "  max_depth: 9\n",
      "  n_estimators: 206\n",
      "  subsample: 0.9118764001091078\n",
      "  XGBoost         | F1: 0.6741 | Acc: 0.8006 | Prec: 0.7406 | Rec: 0.6185 | Time: 14.0s\n",
      "Optimizing ExtraTrees with hyperparameter tuning...\n",
      "Best parameters for ExtraTrees:\n",
      "  max_depth: 20\n",
      "  max_features: None\n",
      "  min_samples_leaf: 3\n",
      "  min_samples_split: 13\n",
      "  n_estimators: 154\n",
      "  ExtraTrees      | F1: 0.6190 | Acc: 0.7827 | Prec: 0.7448 | Rec: 0.5296 | Time: 7.7s\n",
      "Optimizing KNN with hyperparameter tuning...\n",
      "Best parameters for KNN:\n",
      "  metric: euclidean\n",
      "  n_neighbors: 6\n",
      "  weights: distance\n",
      "  KNN             | F1: 0.5918 | Acc: 0.7667 | Prec: 0.7098 | Rec: 0.5074 | Time: 1.1s\n",
      "Optimizing DecisionTree with hyperparameter tuning...\n",
      "Best parameters for DecisionTree:\n",
      "  criterion: gini\n",
      "  max_depth: 30\n",
      "  min_samples_leaf: 8\n",
      "  min_samples_split: 8\n",
      "  DecisionTree    | F1: 0.5689 | Acc: 0.7278 | Prec: 0.6025 | Rec: 0.5389 | Time: 1.6s\n",
      "\n",
      "Evaluating feature combination: 'shape' (shape: 5)\n",
      "Using all features (no feature selection)\n",
      "Optimizing SVM with hyperparameter tuning...\n",
      "Best parameters for SVM:\n",
      "  C: 124.90802376947249\n",
      "  gamma: 0.1\n",
      "  kernel: rbf\n",
      "  SVM             | F1: 0.5535 | Acc: 0.7710 | Prec: 0.7904 | Rec: 0.4259 | Time: 103.6s\n",
      "Optimizing RandomForest with hyperparameter tuning...\n",
      "Best parameters for RandomForest:\n",
      "  max_depth: 20\n",
      "  max_features: log2\n",
      "  min_samples_leaf: 1\n",
      "  min_samples_split: 5\n",
      "  n_estimators: 149\n",
      "  RandomForest    | F1: 0.6194 | Acc: 0.7815 | Prec: 0.7385 | Rec: 0.5333 | Time: 9.0s\n",
      "Optimizing XGBoost with hyperparameter tuning...\n",
      "Best parameters for XGBoost:\n",
      "  colsample_bytree: 0.8887995089067299\n",
      "  learning_rate: 0.05\n",
      "  max_depth: 6\n",
      "  n_estimators: 376\n",
      "  subsample: 0.8469926038510867\n",
      "  XGBoost         | F1: 0.6053 | Acc: 0.7722 | Prec: 0.7165 | Rec: 0.5241 | Time: 1.4s\n",
      "Optimizing ExtraTrees with hyperparameter tuning...\n",
      "Best parameters for ExtraTrees:\n",
      "  max_depth: 20\n",
      "  max_features: None\n",
      "  min_samples_leaf: 3\n",
      "  min_samples_split: 13\n",
      "  n_estimators: 154\n",
      "  ExtraTrees      | F1: 0.5697 | Acc: 0.7753 | Prec: 0.7876 | Rec: 0.4463 | Time: 4.0s\n",
      "Optimizing KNN with hyperparameter tuning...\n",
      "Best parameters for KNN:\n",
      "  metric: euclidean\n",
      "  n_neighbors: 6\n",
      "  weights: distance\n",
      "  KNN             | F1: 0.5708 | Acc: 0.7531 | Prec: 0.6786 | Rec: 0.4926 | Time: 0.4s\n",
      "Optimizing DecisionTree with hyperparameter tuning...\n",
      "Best parameters for DecisionTree:\n",
      "  criterion: entropy\n",
      "  max_depth: 30\n",
      "  min_samples_leaf: 5\n",
      "  min_samples_split: 2\n",
      "  DecisionTree    | F1: 0.5853 | Acc: 0.7315 | Prec: 0.6031 | Rec: 0.5685 | Time: 0.2s\n",
      "\n",
      "Evaluating feature combination: 'color_channel_texture+texture' (color_channel_texture: 117, texture: 60)\n",
      "Using all features (no feature selection)\n",
      "Optimizing SVM with hyperparameter tuning...\n",
      "Best parameters for SVM:\n",
      "  C: 152.84688768272233\n",
      "  gamma: scale\n",
      "  kernel: rbf\n",
      "★ SVM             | F1: 0.8346 | Acc: 0.8963 | Prec: 0.8908 | Rec: 0.7852 | Time: 72.0s\n",
      "Optimizing RandomForest with hyperparameter tuning...\n",
      "Best parameters for RandomForest:\n",
      "  max_depth: 20\n",
      "  max_features: None\n",
      "  min_samples_leaf: 8\n",
      "  min_samples_split: 5\n",
      "  n_estimators: 459\n",
      "  RandomForest    | F1: 0.6786 | Acc: 0.8117 | Prec: 0.7873 | Rec: 0.5963 | Time: 130.4s\n",
      "Optimizing XGBoost with hyperparameter tuning...\n",
      "Best parameters for XGBoost:\n",
      "  colsample_bytree: 0.9369139098379994\n",
      "  learning_rate: 0.2\n",
      "  max_depth: 4\n",
      "  n_estimators: 301\n",
      "  subsample: 0.9579309401710595\n",
      "  XGBoost         | F1: 0.7099 | Acc: 0.8235 | Prec: 0.7848 | Rec: 0.6481 | Time: 41.7s\n",
      "Optimizing ExtraTrees with hyperparameter tuning...\n",
      "Best parameters for ExtraTrees:\n",
      "  max_depth: 20\n",
      "  max_features: None\n",
      "  min_samples_leaf: 3\n",
      "  min_samples_split: 13\n",
      "  n_estimators: 154\n",
      "  ExtraTrees      | F1: 0.6973 | Acc: 0.8173 | Prec: 0.7785 | Rec: 0.6315 | Time: 13.4s\n",
      "Optimizing KNN with hyperparameter tuning...\n",
      "Best parameters for KNN:\n",
      "  metric: euclidean\n",
      "  n_neighbors: 3\n",
      "  weights: distance\n",
      "  KNN             | F1: 0.6320 | Acc: 0.7772 | Prec: 0.7029 | Rec: 0.5741 | Time: 1.1s\n",
      "Optimizing DecisionTree with hyperparameter tuning...\n",
      "Best parameters for DecisionTree:\n",
      "  criterion: gini\n",
      "  max_depth: 20\n",
      "  min_samples_leaf: 1\n",
      "  min_samples_split: 5\n",
      "  DecisionTree    | F1: 0.5936 | Acc: 0.7346 | Prec: 0.6062 | Rec: 0.5815 | Time: 3.8s\n",
      "\n",
      "Evaluating feature combination: 'color_channel_texture+shape' (color_channel_texture: 117, shape: 5)\n",
      "Using all features (no feature selection)\n",
      "Optimizing SVM with hyperparameter tuning...\n",
      "Best parameters for SVM:\n",
      "  C: 152.84688768272233\n",
      "  gamma: scale\n",
      "  kernel: rbf\n",
      "★ SVM             | F1: 0.8420 | Acc: 0.9006 | Prec: 0.8956 | Rec: 0.7944 | Time: 47.3s\n",
      "Optimizing RandomForest with hyperparameter tuning...\n",
      "Best parameters for RandomForest:\n",
      "  max_depth: 30\n",
      "  max_features: sqrt\n",
      "  min_samples_leaf: 1\n",
      "  min_samples_split: 13\n",
      "  n_estimators: 413\n",
      "  RandomForest    | F1: 0.6981 | Acc: 0.8222 | Prec: 0.8043 | Rec: 0.6167 | Time: 46.3s\n",
      "Optimizing XGBoost with hyperparameter tuning...\n",
      "Best parameters for XGBoost:\n",
      "  colsample_bytree: 0.9464704583099741\n",
      "  learning_rate: 0.2\n",
      "  max_depth: 5\n",
      "  n_estimators: 249\n",
      "  subsample: 0.6225646316108401\n",
      "  XGBoost         | F1: 0.7745 | Acc: 0.8605 | Prec: 0.8398 | Rec: 0.7185 | Time: 27.6s\n",
      "Optimizing ExtraTrees with hyperparameter tuning...\n",
      "Best parameters for ExtraTrees:\n",
      "  max_depth: 20\n",
      "  max_features: None\n",
      "  min_samples_leaf: 3\n",
      "  min_samples_split: 13\n",
      "  n_estimators: 154\n",
      "  ExtraTrees      | F1: 0.7218 | Acc: 0.8358 | Prec: 0.8293 | Rec: 0.6389 | Time: 10.7s\n",
      "Optimizing KNN with hyperparameter tuning...\n",
      "Best parameters for KNN:\n",
      "  metric: euclidean\n",
      "  n_neighbors: 3\n",
      "  weights: uniform\n",
      "  KNN             | F1: 0.6734 | Acc: 0.8006 | Prec: 0.7416 | Rec: 0.6167 | Time: 0.9s\n",
      "Optimizing DecisionTree with hyperparameter tuning...\n",
      "Best parameters for DecisionTree:\n",
      "  criterion: entropy\n",
      "  max_depth: 30\n",
      "  min_samples_leaf: 7\n",
      "  min_samples_split: 13\n",
      "  DecisionTree    | F1: 0.6359 | Acc: 0.7660 | Prec: 0.6607 | Rec: 0.6130 | Time: 3.1s\n",
      "\n",
      "Evaluating feature combination: 'texture+shape' (texture: 60, shape: 5)\n",
      "Using all features (no feature selection)\n",
      "Optimizing SVM with hyperparameter tuning...\n",
      "Best parameters for SVM:\n",
      "  C: 92.46782213565524\n",
      "  gamma: 0.01\n",
      "  kernel: rbf\n",
      "  SVM             | F1: 0.7876 | Acc: 0.8735 | Prec: 0.8941 | Rec: 0.7037 | Time: 385.4s\n",
      "Optimizing RandomForest with hyperparameter tuning...\n",
      "Best parameters for RandomForest:\n",
      "  max_depth: 30\n",
      "  max_features: sqrt\n",
      "  min_samples_leaf: 3\n",
      "  min_samples_split: 6\n",
      "  n_estimators: 406\n",
      "  RandomForest    | F1: 0.6446 | Acc: 0.8012 | Prec: 0.7978 | Rec: 0.5407 | Time: 32.7s\n",
      "Optimizing XGBoost with hyperparameter tuning...\n",
      "Best parameters for XGBoost:\n",
      "  colsample_bytree: 0.9369139098379994\n",
      "  learning_rate: 0.2\n",
      "  max_depth: 4\n",
      "  n_estimators: 301\n",
      "  subsample: 0.9579309401710595\n",
      "  XGBoost         | F1: 0.7214 | Acc: 0.8302 | Prec: 0.7964 | Rec: 0.6593 | Time: 13.1s\n",
      "Optimizing ExtraTrees with hyperparameter tuning...\n",
      "Best parameters for ExtraTrees:\n",
      "  max_depth: 20\n",
      "  max_features: None\n",
      "  min_samples_leaf: 3\n",
      "  min_samples_split: 13\n",
      "  n_estimators: 154\n",
      "  ExtraTrees      | F1: 0.6564 | Acc: 0.8068 | Prec: 0.8059 | Rec: 0.5537 | Time: 7.0s\n",
      "Optimizing KNN with hyperparameter tuning...\n",
      "Best parameters for KNN:\n",
      "  metric: euclidean\n",
      "  n_neighbors: 3\n",
      "  weights: distance\n",
      "  KNN             | F1: 0.6421 | Acc: 0.7901 | Prec: 0.7439 | Rec: 0.5648 | Time: 1.0s\n",
      "Optimizing DecisionTree with hyperparameter tuning...\n",
      "Best parameters for DecisionTree:\n",
      "  criterion: entropy\n",
      "  max_depth: 30\n",
      "  min_samples_leaf: 7\n",
      "  min_samples_split: 13\n",
      "  DecisionTree    | F1: 0.6258 | Acc: 0.7586 | Prec: 0.6475 | Rec: 0.6056 | Time: 1.8s\n",
      "\n",
      "Evaluating feature combination: 'color_channel_texture+texture+shape' (color_channel_texture: 117, texture: 60, shape: 5)\n",
      "Using all features (no feature selection)\n",
      "Optimizing SVM with hyperparameter tuning...\n",
      "Best parameters for SVM:\n",
      "  C: 152.84688768272233\n",
      "  gamma: scale\n",
      "  kernel: rbf\n",
      "★ SVM             | F1: 0.8597 | Acc: 0.9117 | Prec: 0.9144 | Rec: 0.8111 | Time: 38.4s\n",
      "Optimizing RandomForest with hyperparameter tuning...\n",
      "Best parameters for RandomForest:\n",
      "  max_depth: 30\n",
      "  max_features: sqrt\n",
      "  min_samples_leaf: 1\n",
      "  min_samples_split: 13\n",
      "  n_estimators: 413\n",
      "  RandomForest    | F1: 0.6913 | Acc: 0.8198 | Prec: 0.8054 | Rec: 0.6056 | Time: 61.7s\n",
      "Optimizing XGBoost with hyperparameter tuning...\n",
      "Best parameters for XGBoost:\n",
      "  colsample_bytree: 0.6260206371941118\n",
      "  learning_rate: 0.2\n",
      "  max_depth: 3\n",
      "  n_estimators: 415\n",
      "  subsample: 0.8253152871382157\n",
      "  XGBoost         | F1: 0.7723 | Acc: 0.8599 | Prec: 0.8425 | Rec: 0.7130 | Time: 42.3s\n",
      "Optimizing ExtraTrees with hyperparameter tuning...\n",
      "Best parameters for ExtraTrees:\n",
      "  max_depth: 20\n",
      "  max_features: None\n",
      "  min_samples_leaf: 3\n",
      "  min_samples_split: 13\n",
      "  n_estimators: 154\n",
      "  ExtraTrees      | F1: 0.7128 | Acc: 0.8309 | Prec: 0.8213 | Rec: 0.6296 | Time: 14.0s\n",
      "Optimizing KNN with hyperparameter tuning...\n",
      "Best parameters for KNN:\n",
      "  metric: euclidean\n",
      "  n_neighbors: 3\n",
      "  weights: distance\n",
      "  KNN             | F1: 0.6584 | Acc: 0.7951 | Prec: 0.7407 | Rec: 0.5926 | Time: 1.2s\n",
      "Optimizing DecisionTree with hyperparameter tuning...\n",
      "Best parameters for DecisionTree:\n",
      "  criterion: entropy\n",
      "  max_depth: 30\n",
      "  min_samples_leaf: 7\n",
      "  min_samples_split: 13\n",
      "  DecisionTree    | F1: 0.6486 | Acc: 0.7753 | Prec: 0.6774 | Rec: 0.6222 | Time: 4.6s\n",
      "All models evaluated successfully.\n",
      "\n",
      "BEST MODEL: SVM with color_channel_texture+texture+shape\n",
      "F1 SCORE: 0.8597\n",
      "ACCURACY: 0.9117\n",
      "PRECISION: 0.9144\n",
      "RECALL: 0.8111\n",
      "\n",
      "BEST HYPERPARAMETERS:\n",
      "  C: 152.84688768272233\n",
      "  gamma: scale\n",
      "  kernel: rbf\n",
      "\n",
      "TOP 3 MODELS BY F1 SCORE:\n",
      "1. SVM with color_channel_texture+texture+shape\n",
      "   F1: 0.8597 | Acc: 0.9117 | Prec: 0.9144 | Rec: 0.8111\n",
      "2. SVM with color_channel_texture+shape\n",
      "   F1: 0.8420 | Acc: 0.9006 | Prec: 0.8956 | Rec: 0.7944\n",
      "3. SVM with color_channel_texture+texture\n",
      "   F1: 0.8346 | Acc: 0.8963 | Prec: 0.8908 | Rec: 0.7852\n",
      "\n",
      "Classification results and best model saved to disk.\n",
      "Generating confusion matrix...\n",
      "\n",
      "Confusion Matrix:\n",
      "  True Negatives (normal correctly predicted): 1039\n",
      "  False Positives (normal incorrectly as defect): 41\n",
      "  False Negatives (defect incorrectly as normal): 102\n",
      "  True Positives (defect correctly predicted): 438\n",
      "Generating ROC curve...\n",
      "Total execution time: 2929.8 seconds (48.8 minutes)\n",
      "Memory usage: 223.7MB → 489.2MB\n",
      "All results saved to: D:\\iate_project\\ml_output\\ml_classification_results\n"
     ]
    }
   ],
   "execution_count": 1
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
