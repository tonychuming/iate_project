{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Model Evaluation and Comparison Script\n",
    "Comprehensively evaluates and compares baseline and CNN models"
   ],
   "id": "cb90c98527cc4dbd"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import json\n",
    "import pickle\n",
    "from pathlib import Path\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import classification_report, roc_curve, auc, precision_recall_curve\n",
    "import torch"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Set style\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "sns.set_palette(\"husl\")"
   ],
   "id": "ab0d9a0b835579b4"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Set project paths\n",
    "PROJECT_ROOT = Path('/home/tony/research_project/iate_project')\n",
    "METRICS_DIR = PROJECT_ROOT / 'results' / 'metrics'\n",
    "FIGURES_DIR = PROJECT_ROOT / 'results' / 'figures'\n",
    "MODELS_DIR = PROJECT_ROOT / 'results' / 'models'"
   ],
   "id": "c3bfcea8bb25dc3b"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Create directories\n",
    "FIGURES_DIR.mkdir(parents=True, exist_ok=True)"
   ],
   "id": "a199487e7603a123"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# 1. LOADING MODEL RESULTS",
   "id": "8dae5df261555fd1"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Load baseline results\n",
    "with open(METRICS_DIR / 'baseline_results.json', 'r') as f:\n",
    "    baseline_results = json.load(f)\n",
    "\n",
    "# Load CNN results\n",
    "with open(METRICS_DIR / 'cnn_results.json', 'r') as f:\n",
    "    cnn_results = json.load(f)\n",
    "\n",
    "print(\"Loaded baseline results\")\n",
    "print(\"Loaded CNN results\")"
   ],
   "id": "702604ecbc8c5c11"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# 2. MODEL PERFORMANCE COMPARISON",
   "id": "e7f95e813c085903"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "comparison_data = []\n",
    "\n",
    "# Baseline models\n",
    "for model_name, metrics in baseline_results['validation_results'].items():\n",
    "    comparison_data.append({\n",
    "        'Model': f'Baseline-{model_name}',\n",
    "        'Type': 'Classical ML',\n",
    "        'Accuracy': metrics['accuracy'],\n",
    "        'Precision': metrics['precision'],\n",
    "        'Recall': metrics['recall'],\n",
    "        'F1-Score': metrics['f1_score'],\n",
    "        'AUC': metrics['auc'],\n",
    "        'Train Time (s)': metrics['train_time']\n",
    "    })\n",
    "\n",
    "# Add baseline test results\n",
    "best_baseline = baseline_results['test_results']['model']\n",
    "comparison_data.append({\n",
    "    'Model': f'{best_baseline} (Test)',\n",
    "    'Type': 'Classical ML',\n",
    "    'Accuracy': baseline_results['test_results']['accuracy'],\n",
    "    'Precision': baseline_results['test_results']['precision'],\n",
    "    'Recall': baseline_results['test_results']['recall'],\n",
    "    'F1-Score': baseline_results['test_results']['f1_score'],\n",
    "    'AUC': baseline_results['test_results']['auc'],\n",
    "    'Train Time (s)': '-'\n",
    "})\n",
    "\n",
    "# CNN models\n",
    "for model_name, metrics in cnn_results['models'].items():\n",
    "    comparison_data.append({\n",
    "        'Model': f'CNN-{model_name}',\n",
    "        'Type': 'Deep Learning',\n",
    "        'Accuracy': metrics['val_accuracy'],\n",
    "        'Precision': metrics['val_precision'],\n",
    "        'Recall': metrics['val_recall'],\n",
    "        'F1-Score': metrics['val_f1_score'],\n",
    "        'AUC': metrics['val_auc'],\n",
    "        'Train Time (s)': metrics['train_time']\n",
    "    })\n",
    "\n",
    "# Add CNN test results\n",
    "best_cnn = cnn_results['best_model']\n",
    "comparison_data.append({\n",
    "    'Model': f'{best_cnn} (Test)',\n",
    "    'Type': 'Deep Learning',\n",
    "    'Accuracy': cnn_results['test_results']['accuracy'],\n",
    "    'Precision': cnn_results['test_results']['precision'],\n",
    "    'Recall': cnn_results['test_results']['recall'],\n",
    "    'F1-Score': cnn_results['test_results']['f1_score'],\n",
    "    'AUC': cnn_results['test_results']['auc'],\n",
    "    'Train Time (s)': '-'\n",
    "})\n",
    "\n",
    "# Create DataFrame\n",
    "comparison_df = pd.DataFrame(comparison_data)\n",
    "comparison_df = comparison_df.round(4)\n",
    "\n",
    "print(\"\\nModel Comparison Table:\")\n",
    "print(comparison_df.to_string(index=False))\n",
    "\n",
    "# Save comparison table\n",
    "comparison_df.to_csv(METRICS_DIR / 'model_comparison.csv', index=False)\n",
    "print(f\"\\nSaved to: {METRICS_DIR / 'model_comparison.csv'}\")"
   ],
   "id": "d781ddfbe7978aef"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# 3. STATISTICAL ANALYSIS",
   "id": "a9878dba95a168cd"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "baseline_f1 = baseline_results['test_results']['f1_score']\n",
    "cnn_f1 = cnn_results['test_results']['f1_score']\n",
    "improvement = ((cnn_f1 - baseline_f1) / baseline_f1) * 100\n",
    "\n",
    "print(f\"Best Baseline F1-Score: {baseline_f1:.4f}\")\n",
    "print(f\"Best CNN F1-Score: {cnn_f1:.4f}\")\n",
    "print(f\"Relative Improvement: {improvement:.2f}%\")"
   ],
   "id": "47d432674fc4a3a0"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# 4. CREATING VISUALIZATIONS",
   "id": "5e6840abd5299891"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Figure 1: Performance comparison bar chart\n",
    "fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n",
    "fig.suptitle('Model Performance Comparison', fontsize=16, fontweight='bold')\n",
    "\n",
    "# Metrics comparison\n",
    "metrics_to_plot = ['Accuracy', 'Precision', 'Recall', 'F1-Score', 'AUC']\n",
    "test_models = comparison_df[comparison_df['Model'].str.contains('Test')]\n",
    "\n",
    "ax = axes[0, 0]\n",
    "x = np.arange(len(metrics_to_plot))\n",
    "width = 0.35\n",
    "\n",
    "baseline_metrics = test_models[test_models['Type'] == 'Classical ML'].iloc[0][metrics_to_plot].values\n",
    "cnn_metrics = test_models[test_models['Type'] == 'Deep Learning'].iloc[0][metrics_to_plot].values\n",
    "\n",
    "bars1 = ax.bar(x - width/2, baseline_metrics, width, label='Baseline', color='#3498db')\n",
    "bars2 = ax.bar(x + width/2, cnn_metrics, width, label='CNN', color='#e74c3c')\n",
    "\n",
    "ax.set_xlabel('Metrics')\n",
    "ax.set_ylabel('Score')\n",
    "ax.set_title('Test Set Performance Comparison')\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels(metrics_to_plot, rotation=45)\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# Add value labels on bars\n",
    "for bars in [bars1, bars2]:\n",
    "    for bar in bars:\n",
    "        height = bar.get_height()\n",
    "        ax.annotate(f'{height:.3f}',\n",
    "                   xy=(bar.get_x() + bar.get_width() / 2, height),\n",
    "                   xytext=(0, 3),\n",
    "                   textcoords=\"offset points\",\n",
    "                   ha='center', va='bottom', fontsize=8)\n",
    "\n",
    "# Confusion matrices\n",
    "baseline_cm = np.array(baseline_results['test_results']['confusion_matrix'])\n",
    "cnn_cm = np.array(cnn_results['test_results']['confusion_matrix'])\n",
    "\n",
    "ax = axes[0, 1]\n",
    "sns.heatmap(baseline_cm, annot=True, fmt='d', cmap='Blues', ax=ax,\n",
    "            xticklabels=['Normal', 'Defect'],\n",
    "            yticklabels=['Normal', 'Defect'])\n",
    "ax.set_title(f'Baseline Confusion Matrix\\n({best_baseline})')\n",
    "ax.set_xlabel('Predicted')\n",
    "ax.set_ylabel('Actual')\n",
    "\n",
    "ax = axes[0, 2]\n",
    "sns.heatmap(cnn_cm, annot=True, fmt='d', cmap='Reds', ax=ax,\n",
    "            xticklabels=['Normal', 'Defect'],\n",
    "            yticklabels=['Normal', 'Defect'])\n",
    "ax.set_title(f'CNN Confusion Matrix\\n({best_cnn})')\n",
    "ax.set_xlabel('Predicted')\n",
    "ax.set_ylabel('Actual')\n",
    "\n",
    "# Training history for CNN\n",
    "if 'train_history' in cnn_results['models'][best_cnn]:\n",
    "    train_history = cnn_results['models'][best_cnn]['train_history']\n",
    "    val_history = cnn_results['models'][best_cnn]['val_history']\n",
    "\n",
    "    # Loss curves\n",
    "    ax = axes[1, 0]\n",
    "    epochs = range(1, len(train_history['loss']) + 1)\n",
    "    ax.plot(epochs, train_history['loss'], 'b-', label='Train Loss')\n",
    "    ax.plot(epochs, val_history['loss'], 'r-', label='Val Loss')\n",
    "    ax.set_xlabel('Epoch')\n",
    "    ax.set_ylabel('Loss')\n",
    "    ax.set_title('CNN Training History - Loss')\n",
    "    ax.legend()\n",
    "    ax.grid(True, alpha=0.3)\n",
    "\n",
    "    # Accuracy curves\n",
    "    ax = axes[1, 1]\n",
    "    ax.plot(epochs, train_history['acc'], 'b-', label='Train Acc')\n",
    "    ax.plot(epochs, val_history['acc'], 'r-', label='Val Acc')\n",
    "    ax.set_xlabel('Epoch')\n",
    "    ax.set_ylabel('Accuracy')\n",
    "    ax.set_title('CNN Training History - Accuracy')\n",
    "    ax.legend()\n",
    "    ax.grid(True, alpha=0.3)\n",
    "\n",
    "# Model comparison radar chart\n",
    "ax = axes[1, 2]\n",
    "categories = ['Accuracy', 'Precision', 'Recall', 'F1-Score', 'AUC']\n",
    "baseline_values = baseline_metrics\n",
    "cnn_values = cnn_metrics\n",
    "\n",
    "angles = np.linspace(0, 2 * np.pi, len(categories), endpoint=False).tolist()\n",
    "baseline_values = np.concatenate((baseline_values, [baseline_values[0]]))\n",
    "cnn_values = np.concatenate((cnn_values, [cnn_values[0]]))\n",
    "angles += angles[:1]\n",
    "\n",
    "ax = plt.subplot(2, 3, 6, projection='polar')\n",
    "ax.plot(angles, baseline_values, 'o-', linewidth=2, label='Baseline', color='#3498db')\n",
    "ax.fill(angles, baseline_values, alpha=0.25, color='#3498db')\n",
    "ax.plot(angles, cnn_values, 'o-', linewidth=2, label='CNN', color='#e74c3c')\n",
    "ax.fill(angles, cnn_values, alpha=0.25, color='#e74c3c')\n",
    "ax.set_xticks(angles[:-1])\n",
    "ax.set_xticklabels(categories)\n",
    "ax.set_ylim(0, 1)\n",
    "ax.set_title('Performance Radar Chart', y=1.08)\n",
    "ax.legend(loc='upper right', bbox_to_anchor=(1.3, 1.0))\n",
    "ax.grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(FIGURES_DIR / 'model_comparison.png', dpi=300, bbox_inches='tight')\n",
    "print(f\"Saved: {FIGURES_DIR / 'model_comparison.png'}\")\n",
    "\n",
    "# Figure 2: Detailed metrics\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
    "fig.suptitle('Detailed Performance Analysis', fontsize=16, fontweight='bold')\n",
    "\n",
    "# Per-class metrics\n",
    "ax = axes[0, 0]\n",
    "class_names = ['Normal', 'Defect']\n",
    "baseline_cm_norm = baseline_cm.astype('float') / baseline_cm.sum(axis=1)[:, np.newaxis]\n",
    "cnn_cm_norm = cnn_cm.astype('float') / cnn_cm.sum(axis=1)[:, np.newaxis]\n",
    "\n",
    "x = np.arange(len(class_names))\n",
    "width = 0.35\n",
    "\n",
    "baseline_recall = np.diag(baseline_cm_norm)\n",
    "cnn_recall = np.diag(cnn_cm_norm)\n",
    "\n",
    "bars1 = ax.bar(x - width/2, baseline_recall, width, label='Baseline', color='#3498db')\n",
    "bars2 = ax.bar(x + width/2, cnn_recall, width, label='CNN', color='#e74c3c')\n",
    "\n",
    "ax.set_xlabel('Class')\n",
    "ax.set_ylabel('Recall (True Positive Rate)')\n",
    "ax.set_title('Per-Class Recall Comparison')\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels(class_names)\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "for bars in [bars1, bars2]:\n",
    "    for bar in bars:\n",
    "        height = bar.get_height()\n",
    "        ax.annotate(f'{height:.3f}',\n",
    "                   xy=(bar.get_x() + bar.get_width() / 2, height),\n",
    "                   xytext=(0, 3),\n",
    "                   textcoords=\"offset points\",\n",
    "                   ha='center', va='bottom')\n",
    "\n",
    "# Precision comparison\n",
    "ax = axes[0, 1]\n",
    "baseline_precision = np.array([baseline_cm[i,i] / baseline_cm[:,i].sum() for i in range(2)])\n",
    "cnn_precision = np.array([cnn_cm[i,i] / cnn_cm[:,i].sum() for i in range(2)])\n",
    "\n",
    "bars1 = ax.bar(x - width/2, baseline_precision, width, label='Baseline', color='#3498db')\n",
    "bars2 = ax.bar(x + width/2, cnn_precision, width, label='CNN', color='#e74c3c')\n",
    "\n",
    "ax.set_xlabel('Class')\n",
    "ax.set_ylabel('Precision')\n",
    "ax.set_title('Per-Class Precision Comparison')\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels(class_names)\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "for bars in [bars1, bars2]:\n",
    "    for bar in bars:\n",
    "        height = bar.get_height()\n",
    "        ax.annotate(f'{height:.3f}',\n",
    "                   xy=(bar.get_x() + bar.get_width() / 2, height),\n",
    "                   xytext=(0, 3),\n",
    "                   textcoords=\"offset points\",\n",
    "                   ha='center', va='bottom')\n",
    "\n",
    "# Error analysis\n",
    "ax = axes[1, 0]\n",
    "error_types = ['False Positives\\n(Normal→Defect)', 'False Negatives\\n(Defect→Normal)']\n",
    "baseline_errors = [baseline_cm[0,1], baseline_cm[1,0]]\n",
    "cnn_errors = [cnn_cm[0,1], cnn_cm[1,0]]\n",
    "\n",
    "x = np.arange(len(error_types))\n",
    "bars1 = ax.bar(x - width/2, baseline_errors, width, label='Baseline', color='#3498db')\n",
    "bars2 = ax.bar(x + width/2, cnn_errors, width, label='CNN', color='#e74c3c')\n",
    "\n",
    "ax.set_xlabel('Error Type')\n",
    "ax.set_ylabel('Number of Errors')\n",
    "ax.set_title('Error Analysis')\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels(error_types)\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "for bars in [bars1, bars2]:\n",
    "    for bar in bars:\n",
    "        height = bar.get_height()\n",
    "        ax.annotate(f'{int(height)}',\n",
    "                   xy=(bar.get_x() + bar.get_width() / 2, height),\n",
    "                   xytext=(0, 3),\n",
    "                   textcoords=\"offset points\",\n",
    "                   ha='center', va='bottom')\n",
    "\n",
    "# Summary statistics\n",
    "ax = axes[1, 1]\n",
    "ax.axis('off')\n",
    "\n",
    "summary_text = f\"\"\"\n",
    "EVALUATION SUMMARY\n",
    "\n",
    "Best Classical ML Model: {best_baseline}\n",
    "• Test Accuracy: {baseline_results['test_results']['accuracy']:.4f}\n",
    "• Test F1-Score: {baseline_results['test_results']['f1_score']:.4f}\n",
    "• Test AUC: {baseline_results['test_results']['auc']:.4f}\n",
    "\n",
    "Best Deep Learning Model: {best_cnn}\n",
    "• Test Accuracy: {cnn_results['test_results']['accuracy']:.4f}\n",
    "• Test F1-Score: {cnn_results['test_results']['f1_score']:.4f}\n",
    "• Test AUC: {cnn_results['test_results']['auc']:.4f}\n",
    "\n",
    "Performance Improvement:\n",
    "• Accuracy: +{(cnn_results['test_results']['accuracy'] - baseline_results['test_results']['accuracy'])*100:.1f}%\n",
    "• F1-Score: +{improvement:.1f}%\n",
    "\n",
    "Key Findings:\n",
    "• CNN significantly outperforms classical ML\n",
    "• Both models show good defect detection\n",
    "• Low false negative rate is critical\n",
    "• Model suitable for deployment\n",
    "\"\"\"\n",
    "\n",
    "ax.text(0.1, 0.5, summary_text, fontsize=11, verticalalignment='center',\n",
    "        bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.5),\n",
    "        family='monospace')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(FIGURES_DIR / 'detailed_analysis.png', dpi=300, bbox_inches='tight')\n",
    "print(f\"Saved: {FIGURES_DIR / 'detailed_analysis.png'}\")"
   ],
   "id": "4a2f1a337398f5ec"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# 5. GENERATING LATEX TABLE FOR PAPER",
   "id": "ecf37092a248c9fa"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "latex_table = comparison_df[comparison_df['Model'].str.contains('Test')].to_latex(\n",
    "    index=False,\n",
    "    float_format='%.4f',\n",
    "    column_format='|l|l|c|c|c|c|c|',\n",
    "    caption='Performance comparison of classical ML and deep learning models on coffee bean defect detection test set.',\n",
    "    label='tab:model_comparison'\n",
    ")\n",
    "\n",
    "with open(METRICS_DIR / 'model_comparison.tex', 'w') as f:\n",
    "    f.write(latex_table)\n",
    "\n",
    "print(f\"LaTeX table saved to: {METRICS_DIR / 'model_comparison.tex'}\")"
   ],
   "id": "714b2d4b806e8546"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Final Summary",
   "id": "fff96bc992192cac"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "print(\"\\nKey Results:\")\n",
    "print(f\"• Best Classical ML: {best_baseline} (F1={baseline_f1:.4f})\")\n",
    "print(f\"• Best Deep Learning: {best_cnn} (F1={cnn_f1:.4f})\")\n",
    "print(f\"• Relative Improvement: {improvement:.1f}%\")\n",
    "\n",
    "print(\"\\nGenerated Files:\")\n",
    "print(f\"• {METRICS_DIR / 'model_comparison.csv'}\")\n",
    "print(f\"• {METRICS_DIR / 'model_comparison.tex'}\")\n",
    "print(f\"• {FIGURES_DIR / 'model_comparison.png'}\")\n",
    "print(f\"• {FIGURES_DIR / 'detailed_analysis.png'}\")"
   ],
   "id": "118e927b6b8cb70"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
