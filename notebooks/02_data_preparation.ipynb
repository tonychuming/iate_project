{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Data Preparation Script for Coffee Bean Defect Detection\n",
    "Creates stratified train/validation/test splits and saves indices"
   ],
   "id": "442a890b4358046e"
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-08-14T17:12:13.405503Z",
     "start_time": "2025-08-14T17:12:13.331818Z"
    }
   },
   "source": [
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import shutil\n",
    "import json\n",
    "import pickle\n",
    "from pathlib import Path\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold\n",
    "from collections import defaultdict\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "RANDOM_SEED = 42\n",
    "np.random.seed(RANDOM_SEED)"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-14T17:12:13.517339Z",
     "start_time": "2025-08-14T17:12:13.515131Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Set project paths\n",
    "PROJECT_ROOT = Path('/home/tony/research_project/iate_project')\n",
    "DATA_DIR = PROJECT_ROOT / 'data' / 'raw'\n",
    "PROCESSED_DIR = PROJECT_ROOT / 'data' / 'processed'\n",
    "SPLITS_DIR = PROJECT_ROOT / 'data' / 'splits'"
   ],
   "id": "25863ca32e064dc8",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-14T17:12:13.566121Z",
     "start_time": "2025-08-14T17:12:13.563458Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Create directories\n",
    "for dir_path in [PROCESSED_DIR / 'train', PROCESSED_DIR / 'val', PROCESSED_DIR / 'test', SPLITS_DIR]:\n",
    "    dir_path.mkdir(parents=True, exist_ok=True)"
   ],
   "id": "1baca7e1e6ff99a5",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# 1. COLLECTING IMAGE PATHS",
   "id": "c4afdf52c087f14c"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-14T17:12:13.651768Z",
     "start_time": "2025-08-14T17:12:13.609268Z"
    }
   },
   "cell_type": "code",
   "source": [
    "all_image_paths = []\n",
    "all_labels = []\n",
    "all_metadata = []\n",
    "\n",
    "for class_idx, class_name in enumerate(['normal', 'defect']):\n",
    "    class_path = DATA_DIR / class_name\n",
    "    if not class_path.exists():\n",
    "        print(f\"Warning: {class_path} does not exist\")\n",
    "        continue\n",
    "\n",
    "    for process_method in ['dry', 'honey', 'wet']:\n",
    "        process_path = class_path / process_method\n",
    "        if not process_path.exists():\n",
    "            continue\n",
    "\n",
    "        for roast_level in ['dark', 'medium', 'light']:\n",
    "            roast_path = process_path / roast_level\n",
    "            if not roast_path.exists():\n",
    "                continue\n",
    "\n",
    "            # Get all images\n",
    "            image_files = [p for p in roast_path.iterdir() if p.is_file() and p.suffix.lower() in {'.jpg', '.jpeg', '.png'}]\n",
    "\n",
    "            for img_file in image_files:\n",
    "                all_image_paths.append(str(img_file))\n",
    "                all_labels.append(class_idx)  # 0 for Normal, 1 for Defect\n",
    "                all_metadata.append({\n",
    "                    'path': str(img_file),\n",
    "                    'class': class_name,\n",
    "                    'class_idx': class_idx,\n",
    "                    'process': process_method,\n",
    "                    'roast': roast_level,\n",
    "                    'filename': img_file.name\n",
    "                })\n",
    "\n",
    "print(f\"Total images collected: {len(all_image_paths)}\")\n",
    "print(f\"Normal images: {all_labels.count(0)}\")\n",
    "print(f\"Defect images: {all_labels.count(1)}\")"
   ],
   "id": "db6340e719956c8a",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total images collected: 5400\n",
      "Normal images: 3600\n",
      "Defect images: 1800\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-14T17:12:13.667763Z",
     "start_time": "2025-08-14T17:12:13.663591Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Convert to numpy arrays\n",
    "all_image_paths = np.array(all_image_paths)\n",
    "all_labels = np.array(all_labels)"
   ],
   "id": "9a92626b9c714e1e",
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# 2. CREATING STRATIFIED SPLITS",
   "id": "6025743bf86220b8"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-14T17:12:13.722601Z",
     "start_time": "2025-08-14T17:12:13.714202Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Split ratios\n",
    "TRAIN_RATIO = 0.70\n",
    "VAL_RATIO = 0.15\n",
    "TEST_RATIO = 0.15\n",
    "\n",
    "# First split: separate test set\n",
    "X_temp, X_test, y_temp, y_test, idx_temp, idx_test = train_test_split(\n",
    "    all_image_paths, all_labels, np.arange(len(all_labels)),\n",
    "    test_size=TEST_RATIO,\n",
    "    stratify=all_labels,\n",
    "    random_state=RANDOM_SEED\n",
    ")\n",
    "\n",
    "# Second split: separate train and validation from temp\n",
    "val_size_adjusted = VAL_RATIO / (TRAIN_RATIO + VAL_RATIO)\n",
    "X_train, X_val, y_train, y_val, idx_train, idx_val = train_test_split(\n",
    "    X_temp, y_temp, idx_temp,\n",
    "    test_size=val_size_adjusted,\n",
    "    stratify=y_temp,\n",
    "    random_state=RANDOM_SEED\n",
    ")\n",
    "\n",
    "print(f\"Train set: {len(X_train)} images\")\n",
    "print(f\"  Normal: {np.sum(y_train == 0)} ({np.sum(y_train == 0)/len(y_train)*100:.1f}%)\")\n",
    "print(f\"  Defect: {np.sum(y_train == 1)} ({np.sum(y_train == 1)/len(y_train)*100:.1f}%)\")\n",
    "\n",
    "print(f\"\\nValidation set: {len(X_val)} images\")\n",
    "print(f\"  Normal: {np.sum(y_val == 0)} ({np.sum(y_val == 0)/len(y_val)*100:.1f}%)\")\n",
    "print(f\"  Defect: {np.sum(y_val == 1)} ({np.sum(y_val == 1)/len(y_val)*100:.1f}%)\")\n",
    "\n",
    "print(f\"\\nTest set: {len(X_test)} images\")\n",
    "print(f\"  Normal: {np.sum(y_test == 0)} ({np.sum(y_test == 0)/len(y_test)*100:.1f}%)\")\n",
    "print(f\"  Defect: {np.sum(y_test == 1)} ({np.sum(y_test == 1)/len(y_test)*100:.1f}%)\")\n"
   ],
   "id": "f9184449079be63c",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train set: 3780 images\n",
      "  Normal: 2520 (66.7%)\n",
      "  Defect: 1260 (33.3%)\n",
      "\n",
      "Validation set: 810 images\n",
      "  Normal: 540 (66.7%)\n",
      "  Defect: 270 (33.3%)\n",
      "\n",
      "Test set: 810 images\n",
      "  Normal: 540 (66.7%)\n",
      "  Defect: 270 (33.3%)\n"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# 3. SAVING SPLIT INDICES",
   "id": "5a21bba973f4e417"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-14T17:12:13.791217Z",
     "start_time": "2025-08-14T17:12:13.767020Z"
    }
   },
   "cell_type": "code",
   "source": [
    "splits = {\n",
    "    'train_indices': idx_train.tolist(),\n",
    "    'val_indices': idx_val.tolist(),\n",
    "    'test_indices': idx_test.tolist(),\n",
    "    'train_paths': X_train.tolist(),\n",
    "    'val_paths': X_val.tolist(),\n",
    "    'test_paths': X_test.tolist(),\n",
    "    'train_labels': y_train.tolist(),\n",
    "    'val_labels': y_val.tolist(),\n",
    "    'test_labels': y_test.tolist(),\n",
    "    'metadata': all_metadata,\n",
    "    'random_seed': RANDOM_SEED\n",
    "}\n",
    "\n",
    "# Save as JSON\n",
    "with open(SPLITS_DIR / 'splits.json', 'w') as f:\n",
    "    json.dump(splits, f, indent=2)\n",
    "print(f\"Saved splits to: {SPLITS_DIR / 'splits.json'}\")\n",
    "\n",
    "# Save as pickle for faster loading\n",
    "with open(SPLITS_DIR / 'splits.pkl', 'wb') as f:\n",
    "    pickle.dump(splits, f)\n",
    "print(f\"Saved splits to: {SPLITS_DIR / 'splits.pkl'}\")"
   ],
   "id": "c7c226139d460839",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved splits to: /home/tony/research_project/iate_project/data/splits/splits.json\n",
      "Saved splits to: /home/tony/research_project/iate_project/data/splits/splits.pkl\n"
     ]
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# 4. CREATING CROSS-VALIDATION SPLITS",
   "id": "975904ac946ad2ad"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-14T17:12:13.828012Z",
     "start_time": "2025-08-14T17:12:13.819854Z"
    }
   },
   "cell_type": "code",
   "source": [
    "skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=RANDOM_SEED)\n",
    "cv_splits = []\n",
    "\n",
    "for fold_idx, (train_idx, val_idx) in enumerate(skf.split(X_train, y_train)):\n",
    "    cv_splits.append({\n",
    "        'fold': fold_idx,\n",
    "        'train_indices': train_idx.tolist(),\n",
    "        'val_indices': val_idx.tolist(),\n",
    "        'train_size': len(train_idx),\n",
    "        'val_size': len(val_idx)\n",
    "    })\n",
    "    print(f\"Fold {fold_idx + 1}: Train={len(train_idx)}, Val={len(val_idx)}\")\n",
    "\n",
    "# Save CV splits\n",
    "with open(SPLITS_DIR / 'cv_splits.json', 'w') as f:\n",
    "    json.dump(cv_splits, f, indent=2)\n",
    "print(f\"\\nSaved CV splits to: {SPLITS_DIR / 'cv_splits.json'}\")"
   ],
   "id": "dafca82b582a8c09",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1: Train=3024, Val=756\n",
      "Fold 2: Train=3024, Val=756\n",
      "Fold 3: Train=3024, Val=756\n",
      "Fold 4: Train=3024, Val=756\n",
      "Fold 5: Train=3024, Val=756\n",
      "\n",
      "Saved CV splits to: /home/tony/research_project/iate_project/data/splits/cv_splits.json\n"
     ]
    }
   ],
   "execution_count": 8
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# 5. GENERATING DATASET STATISTICS",
   "id": "a078d4400ce22637"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-14T17:12:13.874318Z",
     "start_time": "2025-08-14T17:12:13.871914Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def _class_key(x):\n",
    "    if isinstance(x, (int, np.integer)):\n",
    "        return 'Normal' if x == 0 else 'Defect'\n",
    "    s = str(x).lower()\n",
    "    if s == 'normal': return 'Normal'\n",
    "    if s == 'defect': return 'Defect'\n",
    "    return s.title()"
   ],
   "id": "f65f0ebd8beaa899",
   "outputs": [],
   "execution_count": 9
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-14T17:12:13.930043Z",
     "start_time": "2025-08-14T17:12:13.921885Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Process distribution analysis\n",
    "process_dist = defaultdict(lambda: {'train': {'Normal': 0, 'Defect': 0},\n",
    "                                    'val': {'Normal': 0, 'Defect': 0},\n",
    "                                    'test': {'Normal': 0, 'Defect': 0}})\n",
    "\n",
    "roast_dist = defaultdict(lambda: {'train': {'Normal': 0, 'Defect': 0},\n",
    "                                  'val': {'Normal': 0, 'Defect': 0},\n",
    "                                  'test': {'Normal': 0, 'Defect': 0}})\n",
    "\n",
    "# Analyze train set\n",
    "for idx in idx_train:\n",
    "    meta = all_metadata[idx]\n",
    "    process_dist[meta['process']]['train'][_class_key(meta['class'])] += 1\n",
    "    roast_dist[meta['roast']]['train'][_class_key(meta['class'])] += 1\n",
    "\n",
    "# Analyze val set\n",
    "for idx in idx_val:\n",
    "    meta = all_metadata[idx]\n",
    "    process_dist[meta['process']]['val'][_class_key(meta['class'])] += 1\n",
    "    roast_dist[meta['roast']]['val'][_class_key(meta['class'])] += 1\n",
    "\n",
    "# Analyze test set\n",
    "for idx in idx_test:\n",
    "    meta = all_metadata[idx]\n",
    "    process_dist[meta['process']]['test'][_class_key(meta['class'])] += 1\n",
    "    roast_dist[meta['roast']]['test'][_class_key(meta['class'])] += 1\n",
    "\n",
    "# Create statistics summary\n",
    "stats = {\n",
    "    'total_images': len(all_image_paths),\n",
    "    'class_distribution': {\n",
    "        'Normal': all_labels.tolist().count(0),\n",
    "        'Defect': all_labels.tolist().count(1)\n",
    "    },\n",
    "    'split_sizes': {\n",
    "        'train': len(X_train),\n",
    "        'val': len(X_val),\n",
    "        'test': len(X_test)\n",
    "    },\n",
    "    'split_ratios': {\n",
    "        'train': len(X_train) / len(all_image_paths),\n",
    "        'val': len(X_val) / len(all_image_paths),\n",
    "        'test': len(X_test) / len(all_image_paths)\n",
    "    },\n",
    "    'process_distribution': dict(process_dist),\n",
    "    'roast_distribution': dict(roast_dist),\n",
    "    'class_balance': {\n",
    "        'train': {\n",
    "            'Normal': np.sum(y_train == 0) / len(y_train),\n",
    "            'Defect': np.sum(y_train == 1) / len(y_train)\n",
    "        },\n",
    "        'val': {\n",
    "            'Normal': np.sum(y_val == 0) / len(y_val),\n",
    "            'Defect': np.sum(y_val == 1) / len(y_val)\n",
    "        },\n",
    "        'test': {\n",
    "            'Normal': np.sum(y_test == 0) / len(y_test),\n",
    "            'Defect': np.sum(y_test == 1) / len(y_test)\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "# Save statistics\n",
    "with open(SPLITS_DIR / 'dataset_stats.json', 'w') as f:\n",
    "    json.dump(stats, f, indent=2)\n",
    "print(f\"Saved statistics to: {SPLITS_DIR / 'dataset_stats.json'}\")"
   ],
   "id": "7f00b4aebf1689a3",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved statistics to: /home/tony/research_project/iate_project/data/splits/dataset_stats.json\n"
     ]
    }
   ],
   "execution_count": 10
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# 6. CREATING FILE LISTS",
   "id": "5bcb353af2f91b69"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-14T17:12:13.982836Z",
     "start_time": "2025-08-14T17:12:13.975641Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Train list\n",
    "train_list = []\n",
    "for path, label in zip(X_train, y_train):\n",
    "    train_list.append(f\"{path}\\t{label}\\n\")\n",
    "\n",
    "with open(SPLITS_DIR / 'train_list.txt', 'w') as f:\n",
    "    f.writelines(train_list)\n",
    "print(f\"Created: {SPLITS_DIR / 'train_list.txt'}\")\n",
    "\n",
    "# Val list\n",
    "val_list = []\n",
    "for path, label in zip(X_val, y_val):\n",
    "    val_list.append(f\"{path}\\t{label}\\n\")\n",
    "\n",
    "with open(SPLITS_DIR / 'val_list.txt', 'w') as f:\n",
    "    f.writelines(val_list)\n",
    "print(f\"Created: {SPLITS_DIR / 'val_list.txt'}\")\n",
    "\n",
    "# Test list\n",
    "test_list = []\n",
    "for path, label in zip(X_test, y_test):\n",
    "    test_list.append(f\"{path}\\t{label}\\n\")\n",
    "\n",
    "with open(SPLITS_DIR / 'test_list.txt', 'w') as f:\n",
    "    f.writelines(test_list)\n",
    "print(f\"Created: {SPLITS_DIR / 'test_list.txt'}\")"
   ],
   "id": "6ce9d557110ccb38",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created: /home/tony/research_project/iate_project/data/splits/train_list.txt\n",
      "Created: /home/tony/research_project/iate_project/data/splits/val_list.txt\n",
      "Created: /home/tony/research_project/iate_project/data/splits/test_list.txt\n"
     ]
    }
   ],
   "execution_count": 11
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-14T17:12:14.029496Z",
     "start_time": "2025-08-14T17:12:14.027133Z"
    }
   },
   "cell_type": "code",
   "source": [
    "print(\"\\nSummary:\")\n",
    "print(f\"• Total images: {len(all_image_paths):,}\")\n",
    "print(f\"• Train set: {len(X_train):,} images\")\n",
    "print(f\"• Validation set: {len(X_val):,} images\")\n",
    "print(f\"• Test set: {len(X_test):,} images\")\n",
    "print(f\"• Class balance maintained across all splits\")\n",
    "print(f\"• 5-fold CV splits created for robust evaluation\")"
   ],
   "id": "6340c1e03d72cba2",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Summary:\n",
      "• Total images: 5,400\n",
      "• Train set: 3,780 images\n",
      "• Validation set: 810 images\n",
      "• Test set: 810 images\n",
      "• Class balance maintained across all splits\n",
      "• 5-fold CV splits created for robust evaluation\n"
     ]
    }
   ],
   "execution_count": 12
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-14T17:12:14.082102Z",
     "start_time": "2025-08-14T17:12:14.079332Z"
    }
   },
   "cell_type": "code",
   "source": [
    "print(\"\\nFiles created:\")\n",
    "print(f\"• {SPLITS_DIR / 'splits.json'}\")\n",
    "print(f\"• {SPLITS_DIR / 'splits.pkl'}\")\n",
    "print(f\"• {SPLITS_DIR / 'cv_splits.json'}\")\n",
    "print(f\"• {SPLITS_DIR / 'dataset_stats.json'}\")\n",
    "print(f\"• {SPLITS_DIR / 'train_list.txt'}\")\n",
    "print(f\"• {SPLITS_DIR / 'val_list.txt'}\")\n",
    "print(f\"• {SPLITS_DIR / 'test_list.txt'}\")"
   ],
   "id": "3d1767f8fc58b20a",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Files created:\n",
      "• /home/tony/research_project/iate_project/data/splits/splits.json\n",
      "• /home/tony/research_project/iate_project/data/splits/splits.pkl\n",
      "• /home/tony/research_project/iate_project/data/splits/cv_splits.json\n",
      "• /home/tony/research_project/iate_project/data/splits/dataset_stats.json\n",
      "• /home/tony/research_project/iate_project/data/splits/train_list.txt\n",
      "• /home/tony/research_project/iate_project/data/splits/val_list.txt\n",
      "• /home/tony/research_project/iate_project/data/splits/test_list.txt\n"
     ]
    }
   ],
   "execution_count": 13
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
