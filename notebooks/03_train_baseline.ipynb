{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Classical Machine Learning Baseline for Coffee Bean Defect Detection\n",
    "Extracts hand-crafted features and trains traditional classifiers"
   ],
   "id": "749265e2948cb592"
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-08-15T02:48:16.580618Z",
     "start_time": "2025-08-15T02:48:16.291893Z"
    }
   },
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "import pickle\n",
    "import json\n",
    "import time\n",
    "from pathlib import Path\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support, confusion_matrix, roc_auc_score\n",
    "from skimage.feature import graycomatrix, graycoprops, local_binary_pattern\n",
    "import mahotas\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "# Set random seed\n",
    "RANDOM_SEED = 42\n",
    "np.random.seed(RANDOM_SEED)"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-15T02:48:16.696258Z",
     "start_time": "2025-08-15T02:48:16.694227Z"
    }
   },
   "cell_type": "code",
   "source": "EPS = 1e-6",
   "id": "4097fcab25f3181f",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-15T02:48:16.745622Z",
     "start_time": "2025-08-15T02:48:16.743673Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Set project paths\n",
    "PROJECT_ROOT = Path('/home/tony/research_project/iate_project')\n",
    "SPLITS_DIR = PROJECT_ROOT / 'data' / 'splits'\n",
    "RESULTS_DIR = PROJECT_ROOT / 'results'\n",
    "MODELS_DIR = RESULTS_DIR / 'models'\n",
    "METRICS_DIR = RESULTS_DIR / 'metrics'"
   ],
   "id": "99cf2bc11d87ebce",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-15T02:48:16.793490Z",
     "start_time": "2025-08-15T02:48:16.791370Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Create directories\n",
    "MODELS_DIR.mkdir(parents=True, exist_ok=True)\n",
    "METRICS_DIR.mkdir(parents=True, exist_ok=True)"
   ],
   "id": "4d372a63df26911a",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# 1. LOADING DATA SPLITS",
   "id": "8ea84755aa1cbdfc"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-15T02:48:16.848162Z",
     "start_time": "2025-08-15T02:48:16.838847Z"
    }
   },
   "cell_type": "code",
   "source": [
    "with open(SPLITS_DIR / 'splits.pkl', 'rb') as f:\n",
    "    splits = pickle.load(f)\n",
    "\n",
    "train_paths = splits['train_paths']\n",
    "train_labels = splits['train_labels']\n",
    "val_paths = splits['val_paths']\n",
    "val_labels = splits['val_labels']\n",
    "test_paths = splits['test_paths']\n",
    "test_labels = splits['test_labels']\n",
    "\n",
    "print(f\"Train: {len(train_paths)} images\")\n",
    "print(f\"Validation: {len(val_paths)} images\")\n",
    "print(f\"Test: {len(test_paths)} images\")"
   ],
   "id": "1ed9a736e7a3b1a4",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: 3780 images\n",
      "Validation: 810 images\n",
      "Test: 810 images\n"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# 2. DEFINING FEATURE EXTRACTORS",
   "id": "bac426296e3288d5"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-15T02:48:16.892250Z",
     "start_time": "2025-08-15T02:48:16.889089Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def extract_color_features(image):\n",
    "    \"\"\"Extract color histogram and statistical features\"\"\"\n",
    "    features = []\n",
    "\n",
    "    # Convert to different color spaces\n",
    "    hsv = cv2.cvtColor(image, cv2.COLOR_BGR2HSV)\n",
    "    lab = cv2.cvtColor(image, cv2.COLOR_BGR2LAB)\n",
    "\n",
    "    # Color histograms for each channel\n",
    "    for i in range(3):\n",
    "        # BGR histogram\n",
    "        hist_bgr = cv2.calcHist([image], [i], None, [32], [0, 256])\n",
    "        features.extend(hist_bgr.flatten())\n",
    "\n",
    "        # HSV histogram\n",
    "        if i == 0:  # Hue\n",
    "            hist_hsv = cv2.calcHist([hsv], [i], None, [32], [0, 180])\n",
    "        else:  # Saturation and Value\n",
    "            hist_hsv = cv2.calcHist([hsv], [i], None, [32], [0, 256])\n",
    "        features.extend(hist_hsv.flatten())\n",
    "\n",
    "    # Color moments (mean, std, skewness)\n",
    "    for channel in cv2.split(image):\n",
    "        mu = float(np.mean(channel))\n",
    "        sigma = float(np.std(channel))\n",
    "        z3 = 0.0 if sigma < EPS else float(np.mean(((channel - mu) / (sigma + EPS)) ** 3))\n",
    "        features.extend([mu, sigma, abs(z3)])\n",
    "\n",
    "\n",
    "    # LAB color statistics\n",
    "    for channel in cv2.split(lab):\n",
    "        features.append(np.mean(channel))\n",
    "        features.append(np.std(channel))\n",
    "\n",
    "    return features"
   ],
   "id": "9c7cd3b62e264537",
   "outputs": [],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-15T02:48:16.938619Z",
     "start_time": "2025-08-15T02:48:16.935655Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def extract_texture_features(image):\n",
    "    \"\"\"Extract texture features using GLCM, LBP, and Haralick\"\"\"\n",
    "    features = []\n",
    "\n",
    "    # Convert to grayscale\n",
    "    gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "    # GLCM features\n",
    "    distances = [1, 3, 5]\n",
    "    angles = [0, np.pi/4, np.pi/2, 3*np.pi/4]\n",
    "    glcm = graycomatrix(gray, distances=distances, angles=angles,\n",
    "                        levels=256, symmetric=True, normed=True)\n",
    "\n",
    "    # GLCM properties\n",
    "    props = ['contrast', 'dissimilarity', 'homogeneity', 'energy', 'correlation']\n",
    "    for prop in props:\n",
    "        glcm_prop = graycoprops(glcm, prop)\n",
    "        features.extend(glcm_prop.flatten())\n",
    "\n",
    "    # Local Binary Pattern\n",
    "    radius = 3\n",
    "    n_points = 8 * radius\n",
    "    lbp = local_binary_pattern(gray, n_points, radius, method='uniform')\n",
    "    lbp_hist, _ = np.histogram(lbp.ravel(), bins=np.arange(0, n_points + 3),\n",
    "                               range=(0, n_points + 2))\n",
    "    features.extend(lbp_hist)\n",
    "\n",
    "    # Haralick texture features\n",
    "    try:\n",
    "        haralick = mahotas.features.haralick(gray).mean(axis=0)\n",
    "        features.extend(haralick)\n",
    "    except:\n",
    "        features.extend([0] * 13)  # 13 Haralick features\n",
    "\n",
    "    return features"
   ],
   "id": "9ecea4f760449e11",
   "outputs": [],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-15T02:48:16.986223Z",
     "start_time": "2025-08-15T02:48:16.983123Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def extract_shape_features(image):\n",
    "    \"\"\"Extract shape and edge features\"\"\"\n",
    "    features = []\n",
    "\n",
    "    # Convert to grayscale\n",
    "    gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "    # Edge detection\n",
    "    edges = cv2.Canny(gray, 50, 150)\n",
    "    features.append(np.sum(edges > 0))  # Edge pixel count\n",
    "    features.append(np.sum(edges > 0) / edges.size)  # Edge density\n",
    "\n",
    "    # Contour features\n",
    "    contours, _ = cv2.findContours(edges, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "    if contours:\n",
    "        # Find largest contour\n",
    "        largest_contour = max(contours, key=cv2.contourArea)\n",
    "\n",
    "        # Contour area and perimeter\n",
    "        area = cv2.contourArea(largest_contour)\n",
    "        perimeter = cv2.arcLength(largest_contour, True)\n",
    "        features.extend([area, perimeter])\n",
    "\n",
    "        # Circularity\n",
    "        if perimeter > 0:\n",
    "            circularity = 4 * np.pi * area / (perimeter ** 2)\n",
    "            features.append(circularity)\n",
    "        else:\n",
    "            features.append(0)\n",
    "\n",
    "        # Bounding box\n",
    "        x, y, w, h = cv2.boundingRect(largest_contour)\n",
    "        aspect_ratio = float(w) / h if h > 0 else 0\n",
    "        extent = float(area) / (w * h) if w * h > 0 else 0\n",
    "        features.extend([aspect_ratio, extent])\n",
    "\n",
    "        # Hu moments\n",
    "        moments = cv2.moments(largest_contour)\n",
    "        hu_moments = cv2.HuMoments(moments).flatten()\n",
    "        features.extend(hu_moments)\n",
    "    else:\n",
    "        features.extend([0] * 14)  # Fill with zeros if no contours\n",
    "\n",
    "    return features"
   ],
   "id": "d9861a730176b314",
   "outputs": [],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-15T02:48:17.034424Z",
     "start_time": "2025-08-15T02:48:17.031488Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def extract_frequency_features(image):\n",
    "    \"\"\"Extract frequency domain features using FFT\"\"\"\n",
    "    features = []\n",
    "\n",
    "    # Convert to grayscale\n",
    "    gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "    # Apply FFT\n",
    "    f_transform = np.fft.fft2(gray)\n",
    "    f_shift = np.fft.fftshift(f_transform)\n",
    "    magnitude_spectrum = np.abs(f_shift)\n",
    "\n",
    "    # Radial profile\n",
    "    center = np.array(magnitude_spectrum.shape) // 2\n",
    "    y, x = np.ogrid[:magnitude_spectrum.shape[0], :magnitude_spectrum.shape[1]]\n",
    "    r = np.sqrt((x - center[1])**2 + (y - center[0])**2).astype(int)\n",
    "\n",
    "    # Binned radial average\n",
    "    bins = np.arange(0, r.max() + 1, 5)\n",
    "    radial_prof = []\n",
    "    for i in range(len(bins) - 1):\n",
    "        mask = (r >= bins[i]) & (r < bins[i+1])\n",
    "        radial_prof.append(float(np.mean(magnitude_spectrum[mask])) if np.any(mask) else 0.0)\n",
    "\n",
    "    features.extend(radial_prof[:20])  # Use first 20 bins\n",
    "\n",
    "    # Frequency statistics\n",
    "    features.append(np.mean(magnitude_spectrum))\n",
    "    features.append(np.std(magnitude_spectrum))\n",
    "    features.append(np.max(magnitude_spectrum))\n",
    "\n",
    "    return features"
   ],
   "id": "f3ee707ece67abf8",
   "outputs": [],
   "execution_count": 9
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-15T02:48:17.082240Z",
     "start_time": "2025-08-15T02:48:17.079670Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def extract_all_features(image_path):\n",
    "    \"\"\"Extract all features from an image\"\"\"\n",
    "    # Read and resize image\n",
    "    image = cv2.imread(image_path)\n",
    "    if image is None:\n",
    "        return None\n",
    "    image = cv2.resize(image, (256, 256))\n",
    "\n",
    "    # Extract features\n",
    "    color_features = extract_color_features(image)\n",
    "    texture_features = extract_texture_features(image)\n",
    "    shape_features = extract_shape_features(image)\n",
    "    freq_features = extract_frequency_features(image)\n",
    "\n",
    "    # Combine all features\n",
    "    all_features = color_features + texture_features + shape_features + freq_features\n",
    "    all_features = np.nan_to_num(all_features, nan=0.0, posinf=0.0, neginf=0.0)\n",
    "\n",
    "    return np.array(all_features, dtype=np.float32)"
   ],
   "id": "d21773a92b06eb75",
   "outputs": [],
   "execution_count": 10
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# 3. EXTRACTING FEATURES FROM TRAINING SET",
   "id": "3c6a0a9500b48d2c"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-15T02:52:02.082649Z",
     "start_time": "2025-08-15T02:48:17.128066Z"
    }
   },
   "cell_type": "code",
   "source": [
    "X_train = []\n",
    "y_train = []\n",
    "for path, label in tqdm(list(zip(train_paths, train_labels)), desc=\"Extracting train features\"):\n",
    "    features = extract_all_features(path)\n",
    "    if features is not None:\n",
    "        X_train.append(features)\n",
    "        y_train.append(label)\n",
    "\n",
    "X_train = np.array(X_train)\n",
    "y_train = np.array(y_train)\n",
    "\n",
    "print(f\"Train features shape: {X_train.shape}\")"
   ],
   "id": "48fca74be7640274",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Extracting train features:   0%|          | 0/3780 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "3ad50102321249a4ab2ab360acd65301"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train features shape: (3780, 343)\n"
     ]
    }
   ],
   "execution_count": 11
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# 4. EXTRACTING FEATURES FROM VALIDATION SET",
   "id": "1db19ceaf3644278"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-15T02:52:49.615223Z",
     "start_time": "2025-08-15T02:52:02.138678Z"
    }
   },
   "cell_type": "code",
   "source": [
    "X_val = []\n",
    "y_val = []  # NEW\n",
    "for path, label in tqdm(list(zip(val_paths, val_labels)), desc=\"Extracting val features\"):\n",
    "    features = extract_all_features(path)\n",
    "    if features is not None:\n",
    "        X_val.append(features)\n",
    "        y_val.append(label)\n",
    "\n",
    "X_val = np.array(X_val)\n",
    "y_val = np.array(y_val)\n",
    "\n",
    "print(f\"Validation features shape: {X_val.shape}\")"
   ],
   "id": "a626c4c60c74d4b1",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Extracting val features:   0%|          | 0/810 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "6179a2b771134d0ba7640eb7cc34bfcf"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation features shape: (810, 343)\n"
     ]
    }
   ],
   "execution_count": 12
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# 5. NORMALIZING FEATURES",
   "id": "1a40b4766d22743b"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-15T02:52:49.643532Z",
     "start_time": "2025-08-15T02:52:49.627812Z"
    }
   },
   "cell_type": "code",
   "source": [
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_val_scaled = scaler.transform(X_val)\n",
    "\n",
    "print(f\"Features normalized using StandardScaler\")\n",
    "print(f\"Mean: {np.mean(X_train_scaled):.4f}, Std: {np.std(X_train_scaled):.4f}\")\n",
    "\n",
    "# Save scaler\n",
    "with open(MODELS_DIR / 'baseline_scaler.pkl', 'wb') as f:\n",
    "    pickle.dump(scaler, f)"
   ],
   "id": "dc036f3d82e26848",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features normalized using StandardScaler\n",
      "Mean: 0.0000, Std: 0.9985\n"
     ]
    }
   ],
   "execution_count": 13
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# 6. TRAINING CLASSIFIERS",
   "id": "fa766f97bbfc3866"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-15T02:53:25.626993Z",
     "start_time": "2025-08-15T02:52:49.679074Z"
    }
   },
   "cell_type": "code",
   "source": [
    "classifiers = {\n",
    "    'RandomForest': RandomForestClassifier(\n",
    "        n_estimators=200,\n",
    "        max_depth=20,\n",
    "        min_samples_split=10,\n",
    "        min_samples_leaf=5,\n",
    "        random_state=RANDOM_SEED,\n",
    "        n_jobs=-1\n",
    "    ),\n",
    "    'GradientBoosting': GradientBoostingClassifier(\n",
    "        n_estimators=100,\n",
    "        learning_rate=0.1,\n",
    "        max_depth=5,\n",
    "        random_state=RANDOM_SEED\n",
    "    ),\n",
    "    'SVM': SVC(\n",
    "        kernel='rbf',\n",
    "        C=1.0,\n",
    "        gamma='scale',\n",
    "        probability=True,\n",
    "        random_state=RANDOM_SEED\n",
    "    )\n",
    "}\n",
    "\n",
    "results = {}\n",
    "\n",
    "for clf_name, clf in classifiers.items():\n",
    "    print(f\"\\nTraining {clf_name}...\")\n",
    "    start_time = time.time()\n",
    "\n",
    "    # Train\n",
    "    clf.fit(X_train_scaled, y_train)\n",
    "\n",
    "    # Predict on validation\n",
    "    y_pred = clf.predict(X_val_scaled)\n",
    "    y_pred_proba = clf.predict_proba(X_val_scaled)[:, 1] if hasattr(clf, 'predict_proba') else y_pred\n",
    "\n",
    "    # Calculate metrics\n",
    "    accuracy = accuracy_score(y_val, y_pred)\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(y_val, y_pred, average='binary')\n",
    "    auc = roc_auc_score(y_val, y_pred_proba)\n",
    "    cm = confusion_matrix(y_val, y_pred)\n",
    "\n",
    "    train_time = time.time() - start_time\n",
    "\n",
    "    # Store results\n",
    "    results[clf_name] = {\n",
    "        'accuracy': accuracy,\n",
    "        'precision': precision,\n",
    "        'recall': recall,\n",
    "        'f1_score': f1,\n",
    "        'auc': auc,\n",
    "        'confusion_matrix': cm.tolist(),\n",
    "        'train_time': train_time\n",
    "    }\n",
    "\n",
    "    # Save model\n",
    "    model_path = MODELS_DIR / f'baseline_{clf_name.lower()}.pkl'\n",
    "    with open(model_path, 'wb') as f:\n",
    "        pickle.dump(clf, f)\n",
    "\n",
    "    print(f\"  Accuracy: {accuracy:.4f}\")\n",
    "    print(f\"  Precision: {precision:.4f}\")\n",
    "    print(f\"  Recall: {recall:.4f}\")\n",
    "    print(f\"  F1-Score: {f1:.4f}\")\n",
    "    print(f\"  AUC: {auc:.4f}\")\n",
    "    print(f\"  Training time: {train_time:.2f}s\")\n",
    "    print(f\"  Model saved to: {model_path}\")\n",
    "\n",
    "# Select best model based on F1 score\n",
    "best_model_name = max(results, key=lambda x: results[x]['f1_score'])\n",
    "print(f\"\\nBEST MODEL: {best_model_name}\")\n",
    "print(\"-\"*40)\n",
    "print(f\"F1-Score: {results[best_model_name]['f1_score']:.4f}\")"
   ],
   "id": "d3f93c3026d599dd",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training RandomForest...\n",
      "  Accuracy: 0.8877\n",
      "  Precision: 0.8874\n",
      "  Recall: 0.7593\n",
      "  F1-Score: 0.8184\n",
      "  AUC: 0.9533\n",
      "  Training time: 0.59s\n",
      "  Model saved to: /home/tony/research_project/iate_project/results/models/baseline_randomforest.pkl\n",
      "\n",
      "Training GradientBoosting...\n",
      "  Accuracy: 0.8975\n",
      "  Precision: 0.8880\n",
      "  Recall: 0.7926\n",
      "  F1-Score: 0.8376\n",
      "  AUC: 0.9647\n",
      "  Training time: 33.17s\n",
      "  Model saved to: /home/tony/research_project/iate_project/results/models/baseline_gradientboosting.pkl\n",
      "\n",
      "Training SVM...\n",
      "  Accuracy: 0.8815\n",
      "  Precision: 0.8718\n",
      "  Recall: 0.7556\n",
      "  F1-Score: 0.8095\n",
      "  AUC: 0.9451\n",
      "  Training time: 2.18s\n",
      "  Model saved to: /home/tony/research_project/iate_project/results/models/baseline_svm.pkl\n",
      "\n",
      "BEST MODEL: GradientBoosting\n",
      "----------------------------------------\n",
      "F1-Score: 0.8376\n"
     ]
    }
   ],
   "execution_count": 14
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# 7. EVALUATING ON TEST SET",
   "id": "8a3f196a67136a8e"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-15T02:54:12.371159Z",
     "start_time": "2025-08-15T02:53:25.638625Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Extract test features\n",
    "X_test = []\n",
    "y_test = []\n",
    "for path, label in tqdm(list(zip(test_paths, test_labels)), desc=\"Extracting test features\"):\n",
    "    features = extract_all_features(path)\n",
    "    if features is not None:\n",
    "        X_test.append(features)\n",
    "        y_test.append(label)\n",
    "\n",
    "X_test = np.array(X_test)\n",
    "y_test = np.array(y_test)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "print(f\"Test features shape: {X_test.shape}\")\n",
    "\n",
    "# Load best model\n",
    "with open(MODELS_DIR / f'baseline_{best_model_name.lower()}.pkl', 'rb') as f:\n",
    "    best_model = pickle.load(f)\n",
    "\n",
    "# Predict on test set\n",
    "y_test_pred = best_model.predict(X_test_scaled)\n",
    "y_test_proba = best_model.predict_proba(X_test_scaled)[:, 1] if hasattr(best_model, 'predict_proba') else y_test_pred\n",
    "\n",
    "# Calculate test metrics\n",
    "test_accuracy = accuracy_score(y_test, y_test_pred)\n",
    "test_precision, test_recall, test_f1, _ = precision_recall_fscore_support(y_test, y_test_pred, average='binary')\n",
    "test_auc = roc_auc_score(y_test, y_test_proba)\n",
    "test_cm = confusion_matrix(y_test, y_test_pred)\n",
    "\n",
    "print(f\"\\nTest Set Results ({best_model_name}):\")\n",
    "print(f\"  Accuracy: {test_accuracy:.4f}\")\n",
    "print(f\"  Precision: {test_precision:.4f}\")\n",
    "print(f\"  Recall: {test_recall:.4f}\")\n",
    "print(f\"  F1-Score: {test_f1:.4f}\")\n",
    "print(f\"  AUC: {test_auc:.4f}\")\n",
    "\n",
    "print(f\"\\nConfusion Matrix:\")\n",
    "print(f\"  TN: {test_cm[0,0]}, FP: {test_cm[0,1]}\")\n",
    "print(f\"  FN: {test_cm[1,0]}, TP: {test_cm[1,1]}\")"
   ],
   "id": "6d6958ea9f92f77c",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Extracting test features:   0%|          | 0/810 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "1ab1250d32504358844f015a947db185"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test features shape: (810, 343)\n",
      "\n",
      "Test Set Results (GradientBoosting):\n",
      "  Accuracy: 0.9123\n",
      "  Precision: 0.9307\n",
      "  Recall: 0.7963\n",
      "  F1-Score: 0.8583\n",
      "  AUC: 0.9663\n",
      "\n",
      "Confusion Matrix:\n",
      "  TN: 524, FP: 16\n",
      "  FN: 55, TP: 215\n"
     ]
    }
   ],
   "execution_count": 15
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# 8. Save Results",
   "id": "fbf8f04d9916302f"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-15T02:54:12.426474Z",
     "start_time": "2025-08-15T02:54:12.392698Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Save all results\n",
    "all_results = {\n",
    "    'validation_results': results,\n",
    "    'test_results': {\n",
    "        'model': best_model_name,\n",
    "        'accuracy': test_accuracy,\n",
    "        'precision': test_precision,\n",
    "        'recall': test_recall,\n",
    "        'f1_score': test_f1,\n",
    "        'auc': test_auc,\n",
    "        'confusion_matrix': test_cm.tolist()\n",
    "    },\n",
    "    'feature_dimensions': X_train.shape[1],\n",
    "    'feature_types': {\n",
    "        'color': 'Histograms, moments, LAB statistics',\n",
    "        'texture': 'GLCM, LBP, Haralick',\n",
    "        'shape': 'Contours, Hu moments, edge density',\n",
    "        'frequency': 'FFT radial profile'\n",
    "    }\n",
    "}\n",
    "\n",
    "with open(METRICS_DIR / 'baseline_results.json', 'w') as f:\n",
    "    json.dump(all_results, f, indent=2)\n",
    "\n",
    "print(f\"\\nResults saved to: {METRICS_DIR / 'baseline_results.json'}\")\n",
    "\n",
    "# Feature importance for Random Forest\n",
    "if 'RandomForest' in classifiers:\n",
    "    rf_model = classifiers['RandomForest']\n",
    "    feature_importance = rf_model.feature_importances_\n",
    "\n",
    "    # Save feature importance\n",
    "    np.save(MODELS_DIR / 'baseline_feature_importance.npy', feature_importance)\n",
    "    print(f\"\\nFeature importance saved to: {MODELS_DIR / 'baseline_feature_importance.npy'}\")\n",
    "\n",
    "    # Print top 10 important features\n",
    "    top_indices = np.argsort(feature_importance)[-10:][::-1]\n",
    "    print(\"\\nTop 10 Important Features:\")\n",
    "    for i, idx in enumerate(top_indices):\n",
    "        print(f\"  {i+1}. Feature {idx}: {feature_importance[idx]:.4f}\")"
   ],
   "id": "151c0d4293f75a4e",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Results saved to: /home/tony/research_project/iate_project/results/metrics/baseline_results.json\n",
      "\n",
      "Feature importance saved to: /home/tony/research_project/iate_project/results/models/baseline_feature_importance.npy\n",
      "\n",
      "Top 10 Important Features:\n",
      "  1. Feature 310: 0.0891\n",
      "  2. Feature 313: 0.0281\n",
      "  3. Feature 312: 0.0249\n",
      "  4. Feature 314: 0.0228\n",
      "  5. Feature 263: 0.0174\n",
      "  6. Feature 311: 0.0146\n",
      "  7. Feature 315: 0.0144\n",
      "  8. Feature 266: 0.0132\n",
      "  9. Feature 200: 0.0126\n",
      "  10. Feature 93: 0.0125\n"
     ]
    }
   ],
   "execution_count": 16
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-15T02:54:12.450989Z",
     "start_time": "2025-08-15T02:54:12.448760Z"
    }
   },
   "cell_type": "code",
   "source": [
    "print(f\"\\nBest Model: {best_model_name}\")\n",
    "print(f\"Test F1-Score: {test_f1:.4f}\")"
   ],
   "id": "f032919b2d49c94f",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Best Model: GradientBoosting\n",
      "Test F1-Score: 0.8583\n"
     ]
    }
   ],
   "execution_count": 17
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
